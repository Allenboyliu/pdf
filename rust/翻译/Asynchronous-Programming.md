## 目录

### 前言

### 第1部分：异步编程基础

#### 1. 并发与异步编程：详细概述

- **技术要求**
- 多任务处理的进化历程
- 非抢占式多任务
- 抢占式多任务
- 超线程
- 多核处理器
- 你真的是在写同步代码吗？
- 并发与并行
- 我使用的思维模型
- 让我们画出一些与进程经济学的平行关系
- 并发与I/O的关系
- 那操作系统提供的线程怎么办？
- 选择合适的参考框架
- 异步与并发
- 操作系统的角色
- 从操作系统的视角看并发
- 与操作系统合作
- 与操作系统通信
- CPU与操作系统
- 走进兔子洞
- CPU是如何防止我们访问不该访问的内存的？
- 难道我们不能直接在CPU中修改页表吗？
- 中断、固件和I/O
    - 简化概述
    - 中断
    - 固件

#### 总结


## 异步与并发

- 操作系统的角色
- 从操作系统的角度看并发
- 与操作系统合作
- 与操作系统通信
- CPU与操作系统
- 走进兔子洞
- CPU是如何防止我们访问不该访问的内存的？
- 难道我们不能直接在CPU中修改页表吗？
- 中断、固件和I/O
    - 简化概述
    - 中断
    - 固件

#### 总结

---

## 2. 编程语言如何建模异步程序流

- **定义**
- 线程
- 操作系统提供的线程
- 创建新线程需要时间
- 每个线程有自己的栈
- 上下文切换
- 调度
- 将异步操作与操作系统线程解耦的优点
- 示例
- 纤程和绿色线程
- 每个栈有固定的空间
- 上下文切换
- 调度
- 外部函数接口（FFI）
- 基于回调的方法
- 协程：承诺和未来
- 协程与async/await
- **总结**

---

## 3. 理解操作系统支持的事件队列、系统调用和跨平台抽象

- **技术要求**
- 运行Linux示例
- 为什么使用操作系统支持的事件队列？
- 阻塞I/O
- 非阻塞I/O
- 通过epoll/kqueue和IOCP进行事件排队
- 基于就绪的事件队列
- 基于完成的事件队列
- epoll, kqueue和IOCP
- 跨平台事件队列
- 系统调用、FFI和跨平台抽象
## 3. 理解操作系统支持的事件队列、系统调用和跨平台抽象

- **技术要求**
- 运行Linux示例
- 为什么使用操作系统支持的事件队列？
- 阻塞I/O
- 非阻塞I/O
- 通过epoll/kqueue和IOCP进行事件排队
- 基于就绪的事件队列
- 基于完成的事件队列
- epoll, kqueue和IOCP
- 跨平台事件队列
- 系统调用、FFI和跨平台抽象
    - 最低层次的抽象
    - 下一层次的抽象
    - 最高层次的抽象

#### 总结

---

## Part 2: 事件队列与绿色线程

### 4. 创建你自己的事件队列

- **技术要求**
- 设计与epoll简介
- 所有I/O操作都阻塞吗？
- ffi模块
- 位标志与位掩码
- 水平触发与边缘触发事件
- Poll模块
- 主程序
- **总结**

### 5. 创建我们自己的纤程

- **技术要求**
- 如何将仓库与本书一起使用
- 背景信息
    - 指令集、硬件架构与ABI
    - x86-64的System V ABI
    - 简要介绍汇编语言
    - 一个我们可以构建的示例
- 设置我们的项目
- Rust内联汇编宏简介
- 运行我们的示例
- 堆栈
    - 堆栈是什么样的？
    - 堆栈大小
- 实现我们自己的纤程
    - 实现运行时
    - Guard、skip和switch
## ABIs
- x86-64的System V ABI
- 汇编语言简要介绍
- 一个我们可以构建的示例
- 设置我们的项目
- Rust内联汇编宏简介
- 运行我们的示例
- 堆栈
    - 堆栈是什么样的？
    - 堆栈大小
- 实现我们自己的纤程
    - 实现运行时
    - Guard、skip和switch函数
- 完成的思考
- **总结**

---

## Part 3: Rust中的Futures与async/await

### 6. Rust中的Futures

- 什么是Future？
- Leaf Futures
- 非Leaf Futures
- 异步运行时的思维模型
- Rust语言和标准库的处理
- I/O与CPU密集型任务
- **总结**

### 7. 协程和async/await

- **技术要求**
- 无栈协程简介
- 手写协程的示例
- Futures模块
- HTTP模块
- 所有Future都必须是惰性的吗？
- 创建协程
- async/await
- coroutine/wait
- corofy——协程预处理器
- b-async-await——协程/等待转换示例
- c-async-await——并发Future
- 最后的思考
- **总结**

### 8. 运行时、Wakers和反应器-执行器模式

- **技术要求**
- 运行时简介以及为什么需要它们
- 反应器和执行器
- 改进我们的基本示例
- 设计
- 修改当前实现
- 创建一个合适的运行时
- 步骤1 - 改进我们的...
### 步骤 2 - 实现一个合适的执行器
### 步骤 3 - 实现一个合适的反应器
- 实验我们的新运行时
- 一个使用并发的示例
- 同时并行运行多个Future
- **总结**

---

### 9. 协程、自引用结构体与Pinning

- **技术要求**
- 改进我们的示例1 – 变量
- 设置基础示例
- 改进我们的基础示例
- 改进我们的示例2 – 引用
- 改进我们的示例3 – 这…不好…
- 发现自引用结构体
- 什么是move？
- Rust中的Pinning
- Pinning的理论
- 定义
- 固定到堆上
- 固定到栈上
- Pin投影与结构化Pinning
- 改进我们的示例4 – Pinning来救场
- future.rs
- http.rs
- Main.rs
- executor.rs
- **总结**

---

### 10. 创建你自己的运行时

- **技术要求**
- 设置我们的示例
- main.rs
- future.rs
- http.rs
- executor.rs
- reactor.rs
### 实验我们的运行时
- **异步Rust的挑战**
- 显式与隐式反应器实例化
- 人体工程学与效率及灵活性
- 每个人都同意的常见特性
- 异步的drop
- 异步Rust的未来
- **总结**

### 尾声

### 索引

### 你可能喜欢的其他书籍



特定章节的代码位于该章节的文件夹中（例如，ch01）。每个示例被组织为一个单独的 crate。示例名称前的字母指示了书中不同例子的展示顺序。例如，thea-runtime 示例在 b-reactor-executor 示例之前。这种方式使它们按时间顺序排列（至少在大多数系统上是默认的）。一些示例的版本后面带有 -bonus 后缀。这些版本将在书中提到，通常包含一个特定变体的示例，可能很有趣，但与当前主题并无重要关系。

下载示例代码文件
您可以从 GitHub 下载此书的示例代码文件，网址为 https://github.com/PacktPublishing/Asynchronous-Programming-in-Rust。如果代码有更新，它将在 GitHub 仓库中更新。我们还有其他代码包可供查看，来自我们丰富的图书和视频目录，网址为 https://github.com/PacktPublishing/。请查阅！

使用的约定
本书中使用了一些文本约定。

文本中的代码：指文本中的代码词、数据库表名、文件夹名称、文件名、文件扩展名、路径名、虚拟网址、用户输入和 Twitter 帐号。示例：“所以，现在我们创建了自己的 async 运行时，使用 Rust 的 Futures、Waker、Context 和 async/await。”

代码块的格式如下所示：

rust
复制代码

pub trait Future {
    type Output;
    fn poll(&mut self) -> PollState<Self::Output>;
}
引起您注意的代码块的特定部分，相关行或项目会加粗显示：

rust
复制代码

struct Coroutine0 {
    stack: Stack0,
    state: State0,
}
任何命令行输入或输出格式如下：

复制代码

$ cargo run
小贴士或重要说明
以这种方式出现。

联系我们
我们欢迎读者的反馈。

一般反馈：如果您对本书的任何方面有疑问，请通过电子邮件联系我们，地址是 customercare@packtpub.com，并在邮件主题中提及书名。
勘误：虽然我们已尽一切努力确保内容的准确性，但错误确实会发生。如果您在本书中发现错误，我们将非常感激您向我们报告。请访问 www.packtpub.com/support/errata 并填写表格。
盗版：如果您在互联网上发现我们作品的任何非法复制品，请向我们提供位置地址或网站名称。请通过 copyright@packt.com 联系我们，并提供材料的链接。
有意成为作者：如果您对某个主题有专业知识，并且有兴趣写作或贡献一本书，请访问 authors.packtpub.com。
分享您的想法
阅读完《Rust 中的异步编程》后，我们很想听听您的想法！请点击这里直接访问本书的 Amazon 评价页面并分享您的反馈。您的评价对我们和技术社区都很重要，将帮助我们确保提供卓越质量的内容。

下载本书的免费 PDF 副本
感谢您购买本书！您是否喜欢随身阅读，但无法随身携带纸质书籍？您的电子书购买是否与您选择的设备不兼容？别担心，现在每本 Packt 书籍都可以免费获得无 DRM 的 PDF 版本。

随处随在，在您选择的任何设备上阅读。可以直接在应用程序中搜索、复制和粘贴您喜欢的技术书籍中的代码。这些好处不止于此，您还可以独家获得每日发送至您邮箱的折扣、通讯和极好的免费内容。

获取福利的简单步骤
扫描二维码或访问以下链接。

第一部分：异步编程基础
在本部分中，您将获得对并发和异步编程的全面介绍。我们还将探索各种编程语言用于建模异步性的技术，审视其中最流行的技术，并涵盖与每种技术相关的一些优缺点。最后，我们将解释操作系统支持的事件队列的概念，如 epoll、kqueue 和 IOCP，详细说明如何使用系统调用与操作系统进行交互，并解决创建跨平台抽象（如 mio）时遇到的挑战。本节包含以下章节：

第1章：并发与异步编程：详细概述
第2章：编程语言如何建模异��程序流
第3章：理解操作系统支持的事件队列、系统调用及跨平台抽象
1. 并发与异步编程：详细概述
异步编程是许多程序员认为令人困惑的话题之一。你会觉得自己似乎理解了它，然而随后却意识到这一领域比你想象的要复杂得多。如果你参与讨论，听足够多的演讲，并在互联网上阅读关于该主题的内容，你可能也会看到一些似乎互相矛盾的说法。至少，这描述了我第一次接触这个主题时的感受。

造成这种困惑的原因往往是缺乏上下文，或者作者在没有明确说明的情况下假定了特定的上下文，同时与并发和异步编程相关的术语相对定义较差。

在本章中，我们将覆盖很多内容，并将内容分为以下主要主题：

异步历史
并发与并行
操作系统与 CPU
中断、固件和 I/O
本章的性质较为总体，并不专门关注 Rust 或任何特定的编程语言，但这是我们需要了解的背景信息，以确保大家在接下来的讨论中有相同的基础。好处是，这些知识在无论使用何种编程语言时都是有用的。在我看来，这一事实也使得本章成为本书中最有趣的章节之一。

本章中的代码不多，因此我们轻松开始。现在正是泡一杯茶、放松身心的时候，因为我们将开始这段共同的旅程。

技术要求
所有示例将用 Rust 编写，您有两种选择来运行这些示例：

在 Rust Playground 上编写和运行我们将写的示例
在您的机器上安装 Rust 并本地运行示例（推荐）
阅读本章的理想方式是克隆附带的仓库（[https://github.com/PacktPublishing/Asynchronous-Programming-in-Rust/tree/main/ch01/a-](https://github.com/PacktPublishing/Asynchronous-Programming-in-Rust/tree/main/ch01/a-））。

替代方案4 - 使用两个酒保的并行和异步任务执行
如果你雇佣两个酒保，并要求他们执行替代方案3中描述的工作，但有一个变化：允许他们互相“抢”任务，这样酒保1可以开始倒酒并将啤酒放下静置，而酒保2可以在酒保1忙于倒新订单时进行加满和服务。这样，两位酒保很少会同时忙于工作，因为正在进行的啤酒在准备好时可以及时加满和服务。几乎所有的订单都在最短的时间内完成并服务，让顾客能更快地带着啤酒离开酒吧，并为想要下新订单的顾客腾出空间。

这样，你可以进一步提高吞吐量。尽管仍无法达到理论最大值，但你会非常接近。在开业之夜，你意识到酒保们每小时处理230个订单，总吞吐量达到每小时460瓶啤酒。收入看起来不错，顾客们很高兴，成本保持在最低水平，而你则是这个世界上最奇怪酒吧（不过是个极其高效的酒吧）的一位快乐管理者。

关键要点
并发是更加明智地工作的一种方式。并行则是向问题投入更多资源的一种方式。

并发及其与 I/O 的关系
正如你从我 até 目前为止的写作中可以理解的那样，编写异步代码通常在需要聪明地充分利用你的资源时更有意义。

如果你编写一个努力解决问题的程序，通常并发就没有什么帮助。这正是并行发挥作用的地方，因为如果你能将问题拆分成可以并行处理的部分，它就为你提供了一种投放更多资源的方式。考虑下面这两种并发的不同用例：

当执行 I/O 时，你需要等待某些外部事件发生。
当你需要分散注意力，防止一个任务等待过久。
第一个是经典的 I/O 示例：你必须等待网络调用、数据库查询或其他事情发生，才能推进任务。然而，你有许多任务要做，所以与你其等着的做法是继续在其他地方工作，定期检查任务是否准备好推进，或确保你会在任务准备好时收到通知。

第二个是当有用户界面时常见的情况。假设你只有一个核心。你如何确保在执行其他密集 CPU 任务时，让整个用户界面不会变得无响应呢？你可以每16毫秒停止你正在做的任何任务，运行更新用户界面的任务，然后再恢复你之前的工作。这样，你每秒需停止并恢复任务60次，但你也会拥有一个全响应的用户界面，刷新率约为60赫兹。

操作系统提供的线程呢？
在本书后面讨论处理 I/O 的策略时，我们将更深入地讨论线程，但我在这里也会提到它们。使用操作系统线程来理解并发的一个挑战是，它们似乎被映射到核心上。尽管大多数操作系统会尽量把一个线程映射到一个核心，直到线程数量等于核心数量，但这并不一定是一个正确的思维模型。

一旦我们创建的线程数量超过核心数量，操作系统将会在我们的线程之间切换，并使用其调度程序并发处理每个线程，为每个线程提供一些运行时间。你还必须考虑到你的程序并不是唯一在系统上运行的程序。其他程序也可能会生成多个线程，这意味着线程的数量将远远超过 CPU 上的核心数。

因此，线程可以是一种并行执行任务的手段，但它们也可以是一种实现并发的手段。

正确的参考框架
关于并发的最后一部分，它需要在某种参考框架中定义。

当你编写从你的角度来看完美同步的代码时，停下来想一想，从操作系统的角度来看那是什么样子。操作系统可能根本不会从头到尾运行你的代码。它可能会多次停止和恢复你的进程。CPU 可能在你认为它只专注于你的任务时会被中断并处理一些输入。

因此，同步执行只是一个幻觉。但从你作为程序员的角度来看，它并不是，这一点非常重要：

当我们讨论并发而不提供任何其他上下文时，我们使用你作为程序员和你的代码（你的进程）作为参考框架。如果你在思考并发时没有牢记这一点，事情很快就会变得混乱。

与操作系统的通信
与操作系统的通信是通过我们称之为系统调用（syscall）的机制实现的。我们需要知道如何进行系统调用，并理解当我们想与操作系统合作和沟通时，这为何如此重要。我们还需要了解我们日常使用的基本抽象如何在后台使用系统调用。我们将在第3章进行详细讲解，所以现在先简要介绍一下。

系统调用使用操作系统提供的公共 API，这样我们在“用户空间”编写的程序就可以与操作系统通信。大多数情况下，这些调用对于我们程序员来说已被我们使用的语言或运行时抽象了。

现在，一个系统调用是一个与您正在通信的内核特有的示例，但 UNIX 系列的内核有许多相似之处。UNIX 系统通过 libc 暴露这一点。而 Windows 则使用自己的 API，通常称为 WinAPI，它的操作方式与基于 UNIX 的系统可能有很大的不同。通常，有一种方法可以实现相同的功能。在功能方面，您可能不会注意到太大的差异，但正如我们稍后看到的，尤其是当我们深入了解 epoll、kqueue 和 IOCP 的工作原理时，它们在实现这些功能的方式上可以存在很大差异。

然而，系统调用并不是我们与操作系统交互的唯一方式，接下来的部分我们将看到这一点。

CPU 与操作系统
CPU 是否与操作系统合作？如果在我第一次认为我理解程序如何工作时问我这个问题，我很可能会回答“否”。我们在 CPU 上运行程序，只要知道怎么做，就可以随心所欲。但首先，我并没有认真思考这个问题，除非你了解 CPU 和操作系统是如何协同工作的，否则很难确切知道。

让我意识到我错得很离谱的是一段类似您即将看到的代码。如果您觉得 Rust 中的内联汇编看起来陌生且令人困惑，请暂时不用担心。我们将在本书稍后进行适当的内联汇编介绍。我会逐行解释以下代码，直到您对语法更熟悉为止：

仓库参考： ch01/ac-assembly-dereference/src/main.rs

rust
复制代码

fn main() {
    let t = 100;
    let t_ptr: *const usize = &t;
    let x = dereference(t_ptr);
    println!("{}", x);
}

fn dereference(ptr: *const usize) -> usize {
    let mut res: usize;
    unsafe {
        asm!("mov {0}, [{1}]", out(reg) res, in(reg) ptr)
    };
    res
}
您看到的正是用汇编编写的间接引用函数。mov {0}, [{1}] 这行需要一些解释。{0} 和 {1} 是模板，用于告诉编译器我们正在指代 out(reg) 和 in(reg) 表示的寄存器。数字只是索引，因此如果我们有更多的输入或输出，它们将编号为 {2}、{3} 等。由于我们只指定了 reg 而不是特定寄存器，我们让编译器选择它想使用的寄存器。

mov 指令指示 CPU 从 ptr 指向的内存位置读取前 8 个字节（如果我们在 64 位计算机上），并将其放置在 {0} 所代表的寄存器中。方括号 [] 将 instruct CPU 将该寄存器中的数据视为内存地址，而不是简单地将内存地址复制到 {0}。它会提取该内存位置上的内容并移动过去。

无论如何，我们在这里只是向 CPU 编写指令。没有标准库，没有系统调用；只是原始指令。操作系统根本没有涉及到这个间接引用函数，对吧？

如果您运行这个程序，您会得到期望的结果：100。

现在，如果您保留间接引用函数，但用一个创建指向 99999999999999 地址（我们知道这个地址是无效的）的指针的函数替换 main 函数，代码变为：

rust
复制代码

fn main() {
    let t_ptr = 99999999999999 as *const usize;
    let x = dereference(t_ptr);
    println!("{}", x);
}
现在，如果我们运行它，会得到以下结果：

在 Linux 上的结果：

复制代码

Segmentation fault (core dumped)
在 Windows 上的结果：

复制代码

error: process didn't exit successfully: `target\debug\ac-assembly-dereference.exe` (exit code: 0xc0000005, STATUS_ACCESS_VIOLATION)
我们得到了段错误。这并不令人惊讶，但正如您可能注意到的那样，我们在不同平台上得到的错误是不同的。显然，操作系统以某种方式参与了这一过程。让我们看看这里实际上发生了什么。


走进兔子洞
事实证明，操作系统与 CPU 之间有着大量的合作，但这可能并不是你天真想象的那样。许多现代 CPU 提供了一些操作系统所需的基本基础设施。这些基础设施为我们提供了我们所期望的安全性和稳定性。实际上，大多数高级 CPU 提供的选项远比 Linux、BSD 和 Windows 等操作系统实际使用的要多得多。

这里我想特别提及两个方面：

CPU 如何阻止我们访问不应该访问的内存。
CPU 如何处理异步事件，例如 I/O。
我们将在这里讨论第一个问题，而第二个问题将在下一部分讨论。

CPU 如何防止我们访问不应该访问的内存？
正如我提到的，现代 CPU 架构通过设计定义了一些基本概念。以下是一些示例：

虚拟内存
页表
页故障
异常
特权等级
具体的实现方式会因具体的 CPU 而有所不同，因此我们在此将其概括性处理。

大多数现代 CPU 都配备了内存管理单元（MMU）。这一部分有时甚至与 CPU 同一块晶圆上制造。MMU 的任务是将我们在程序中使用的虚拟地址转换为物理地址。

当操作系统启动一个进程（如我们的程序）时，它会为我们的进程设置一个页表，并确保 CPU 上的一个特殊寄存器指向这个页表。

现在，当我们尝试间接引用 t_ptr 时，这个地址最终会被发送到 MMU 进行转换，MMU 会在页表中查找，并将其转换为内存中的物理地址，以便它可以提取数据。

在第一个情况下，它将指向我们栈中的一个内存地址，该地址保存着值 100。当我们输入 99999999999999 并请求提取存储在该地址的内容（这就是间接引用的功能）时，它在页表中寻找对应的翻译，却找不到。

此时，CPU 将此视为页故障。

在启动时，操作系统向 CPU 提供了一个中断描述符表。这个表具有预定义的格式，操作系统为 CPU 可能遇到的预定义条件提供处理程序。由于操作系统提供了指向处理页故障的函数的指针，当我们尝试间接引用 99999999999999 时，CPU 会跳转到该函数，从而将控制权交给操作系统。

操作系统随后会为我们打印一条友好的消息，告诉我们遇到了它所称的段错误（segmentation fault）。因此，这条消息可能会根据您运行代码的操作系统而有所不同。

我们不能直接更改 CPU 中的页表吗？
这时，特权等级就派上用场了。大多数现代操作系统采用两个环级别：环 0（内核空间）和环 3（用户空间）。

大多数 CPU 拥有比现代操作系统使用的更多环的概念
这是出于历史原因，这也是为什么使用环 0 和环 3（而不是环 1 和环 2）的原因。每个页表中的条目都有关于其额外信息。其中包含有关其所属环的信息。这些信息在您的操作系统启动时设置。

在环 0 中执行的代码几乎具有对外部设备和内存的无限制访问，可以自由更改提供硬件层面安全性的寄存器。而您在环 3 中编写的代码通常对 I/O 和某些 CPU 寄存器（以及指令）具有极其有限的访问权限。试图从环 3 中发出指令或设置寄存器以更改页表将被 CPU 阻止。CPU 会将此视为异常，并跳转到操作系统提供的该异常的处理程序。

这也是您别无选择，只能与操作系统合作并通过系统调用处理 I/O 任务的原因。如果不是这样的情况，系统将不会非常安全。

总结
简而言之：是的，CPU 和操作系统之间有很大的合作。大多数现代桌面 CPU 的设计考虑到了操作系统，因此它们提供了操作系统在启动时依附于的钩子和基础设施。当操作系统生成一个进程时，它也会设置其特权级别，确保普通进程保持在其定义的边界内，以维持稳定性和安全性。

中断、固件和 I/O
我们即将结束本书中的一般计算机科学主题，并将很快开始探索如何走出兔子洞。

这一部分试图将所有内容结合起来，观察整个计算机如何作为一个系统来处理 I/O 和并发。

让我们开始吧！

简化概述
让我们看一下我们从网络卡读取数据时的一些步骤。

中断
如您所知，存在两种类型的中断：

硬件中断
软件中断
它们在本质上是非常不同的��

硬件中断
硬件中断是通过在 IRQ 上发送电信号来产生的。这些硬件线路直接向 CPU 发出信号。

软件中断
软件中断则是由软件发出的，而不是硬件发出的。与硬件中断一样，CPU 会跳转到中断描述符表（IDT），并运行指定中断的处理程序。

固件
固件在我们大多数人眼中并没有得到太多关注；然而，它是我们生活中至关重要的一部分。固件在各种硬件上运行，并以各种奇怪且特殊的方式使我们所编程的计算机正常工作。

现在，固件需要微控制器才能工作。甚至 CPU 也有使其正常工作的固件。这意味着在我们的系统中，存在比我们编程所针对的核心更多的小“CPU”。

为什么这很重要？
好吧，您还记得并发是关于效率的，对吗？既然系统中已有许多 CPU/微控制器在为我们工作，我们写代码时的一个关注点就是不要重复或复制这些工作。

如果网络卡有固件不断检查是否有新数据到达，那么如果让我们的 CPU 也不断检查是否有新数据到达，那将非常浪费。更好的方式是偶尔检查一次，或者更好的是，当数据到达时获得通知。

总结
本章涵盖了很多内容，因此您做得很好，完成了这些基础工作。我们从历史角度了解了 CPU 和操作系统如何演变，以及非抢占式和抢占式多任务之间的区别。我们讨论了并发与并行之间的差异，谈论了操作系统的角色，并了解到系统调用是我们与宿主操作系统交互的主要方式。您还看到了 CPU 和操作系统通过设计为 CPU 一部分的基础设施进行合作的方式。

最后，我们查看了一张关于发出网络调用时会发生什么的图。您知道我们至少有三种不同的方法来处理 I/O 调用执行所需的时间，而我们必须决定以哪种方式来处理这个等待时间。

这一部分涵盖了我们所需的大部分背景信息，以确保在继续之前我们有相同的定义和概述。随着我们在书中的深入，将会有更多的详细内容，而下一章的第一个主题是编程语言如何通过线程、协程和期货模型化异步程序流程。


2. 编程语言如何模型化异步程序流
在上一章中，我们对异步程序流、并发和并行进行了概述。在本章中，我们将缩小范围。具体来说，我们将探讨编程语言和库中模型化并发的不同方式。

需要记住的是，线程、期货、纤维、协程、承诺等都是抽象，它们为我们提供了一种模型化异步程序流的方式。它们各有优缺点，但共同的目标是为程序员提供一种易于使用（同样重要的是，不容易误用）、高效且富有表现力的方式，以创建以非顺序且往往不可预测的方式处理任务的程序。

在这里，缺乏准确的定义同样普遍；许多术语的名称源于某个特定时间的具体实现，但后来被赋予了更普遍的意义，涵盖了同一事物的不同实现和变种。

我们将首先通过它们的相似性来对不同的抽象进行分组，然后再讨论每种抽象的优缺点。我们还会介绍一些将在全书中使用的重要定义，并详细讨论操作系统线程。

我们讨论的主题相对抽象且复杂，所以如果您不能立即理解所有内容，也不要感到沮丧。随着我们在书中的深入，通过处理一些示例，您会逐渐习惯不同的术语和技术，更多的知识会变得清晰。

具体而言，将涵盖以下主题：

定义
操作系统提供的线程
绿色线程/栈满协程/纤维
基于回调的方法
承诺、期货以及 async/await
定义
我们可以将并发操作的抽象大致分为两类：

协作式：这些任务自愿让出控制权，要么通过明确的让步，要么通过调用一个在另一项操作完成之前无法进一步推进时挂起任务的函数（例如发起网络调用）。这些任务通常会让出控制权给某种调度程序。Rust 和 JavaScript 中的 async/await 生成的任务就是这类任务的例子。

非协作式：这些任务不一定自愿让出控制权。在这样的系统中，调度程序必须能够抢先控制正在运行的任务，这意味着调度程序可以停止任务并控制 CPU，即使该任务能够继续工作并推进。操作系统线程和 Goroutines（自 Go 版本 1.14 之后）就是这类任务的例子。

嵌入式系统
嵌入式系统如今比以往任何时候都更为普遍。这种硬件可能没有足够的资源运行操作系统，如果有，它们可能会使用一种与您的需求高度契合的根本不同的操作系统，因为这些系统往往不那么通用，而具有更专门化的特点。

它们对线程的支持和调度特性可能与您在像 Windows 或 Linux 这样的操作系统中所习惯的不同。由于涵盖所有不同设计将是一本独立的书籍，我们将局限于讨论在流行的桌面和服务器 CPU 上运行的 Windows 和 Linux 系统中的线程。

操作系统线程很容易实现和使用。我们只需让操作系统为我们处理所有事务。我们通过为每个要完成的任务生成一个新的操作系统线程，并像平常一样编写代码来实现这一点。我们处理并发的运行时环境就是操作系统本身。除了这些优点之外，您还可以免费获得并行性。然而，直接管理并行性和共享资源也会带来一些缺点和复杂性。

创建新线程所需时间
创建一个新的操作系统线程涉及一些记录和初始化开销，因此虽然在相同进程中切换两个现有线程相当快速，但创建新线程和丢弃不再使用的线程会涉及耗时的工作。如果系统需要创建和丢弃大量线程，这一额外的工作将限制吞吐量。如果有大量的小任务需要并发处理，这在处理大量 I/O 时往往是个问题。

每个线程都有自己的栈
我们将在本书后面详细介绍栈，但现在知道它们占据固定大小的内存就足够了。每个操作系统线程都有自己独立的栈，即使许多系统允许配置这个大小，它们仍然是固定的，无法增长或缩小。毕竟，栈溢出就是由此造成的，如果您将其配置得过小而无法满足正在运行的任务，就会成为一个问题。

如果我们有许多只需要少量栈空间的小任务，但我们预留了比所需更多的栈空间，那么我们将占用大量内存，并可能耗尽可用内存。

上下文切换
正如您现在所知道的，线程和调度程序是紧密相连的。上下文切换发生在 CPU 停止执行一个线程并转到另一个线程时。尽管这个过程经过高度优化，但它仍涉及到存储和恢复寄存器状态，这需要时间。每次您让出控制权给操作系统调度程序时，它可以选择在该 CPU 上调度一个来自不同进程的线程。

您看到，这些系统创建的线程属于一个进程。当您启动一个程序时，它启动一个进程，该进程创建至少一个初始线程，并在该线程中执行您编写的程序。每个进程可以生成多个共享同一地址空间的线程。这意味着在同一进程内的线程可以访问共享内存，并可以访问相同的资源，例如文件和文件句柄。

这样的结果是，当操作系统通过停止一个线程并恢复同一进程中的另一个线程来进行上下文切换时，它不需要保存和恢复与该进程相关的所有状态，仅需保存与该线程相关的状态。

另一方面，当操作系统从与一个进程相关的线程切换到与另一个进程相关的线程时，新进程将使用不同的地址空间，操作系统需要采取措施确保进程“A”不会访问属于进程“B”的数据或资源。如果不这样做，系统的安全性将受到威胁。

因此，结果是可能需要冲刷缓存，并且可能需要保存和恢复更多的状态。在高并发的系统中，当负载增加时，这些上下文切换可能会额外耗时，从而在频繁发生时以某种不可预测的方式限制吞吐量。

调度
操作系统可以以您可能不期望的方式调度任务，每当您让位于操作系统时，您就会与系统上所有其他线程和进程排在同一个队列中。

此外，由于没有保证线程会在与其离开时相同的 CPU 核心上恢复执行，或者两个任务不会并行运行并尝试访问相同的数据，因此您需要同步数据访问，以防止数据竞争和与多核编程相关的其他陷阱。

作为一门语言，Rust 将帮助您防止许多这些陷阱，但同步数据访问将需要额外的工作，并增加此类程序的复杂性。我们常常说，使用操作系统线程来处理并发为我们免费提供了并行性，但在增加复杂性和对正确数据访问同步的需求方面，这并不是免费的。



线程与调度器的关系
如您现在所知，线程和调度器是紧密相连的。上下文切换发生在 CPU 停止执行一个线程并转向另一个线程时。尽管这个过程经过高���优化，但它仍涉及到存储和恢复寄存器状态，这需要时间。每当您将控制权让给操作系统调度程序时，它可以选择在该 CPU 上调度不同进程中的一个线程。

您会看到，这些系统创建的线程属于一个进程。当您启动一个程序时，它会启动一个进程，该进程创建至少一个初始线程，并在该线程中执行您编写的程序。每个进程可以生成多个共享同一地址空间的线程。

这意味着，同一进程内的线程可以访问共享内存，并可以访问相同的资源，例如文件和文件句柄。这样做的一个后果是，当操作系统通过停止一个线程并恢复同一进程中的另一个线程来进行上下文切换时，它不需要保存和恢复与该进程相关的所有状态，只需保存与该线程相关的状态。

另一方面，当操作系统从一个进程相关的线程切换到另一个进程相关的线程时，新进程将使用不同的地址空间，操作系统需要采取措施确保进程“A”不会访问属于进程“B”的数据或资源。如果不这样做，系统将是不安全的。

因此，缓存可能需要被清空，并且可能需要保存和恢复更多的状态。在高并发的系统负载下，这些上下文切换可能会额外耗时，从而在频繁发生时以某种不可预测的方式限制吞吐量。

调度
操作系统可以以您可能不期望的方式调度任务，每当您向操作系统让出控制权时，您会与系统上所有其他线程和进程排在同一个队列中。

此外，由于没有保证线程会在与其离开时相同的 CPU 核心上恢复执行，或者两个任务不会同时运行并尝试访问相同的数据，您需要同步数据访问，以防止数据竞争和与多核编程相关的其他陷阱。

作为一门语言，Rust 将帮助您防止许多这些陷阱，但同步数据访问将需要额外的工作，并增加此类程序的复杂性。我们常常说，使用操作系统线程来处理并发为我们提供了免费的并行性，但在增加复杂性和对正确数据访问同步的需求方面，这并不是免费的。

将异步操作与操作系统线程解耦的优势
将异步操作与线程的概念解耦有很多好处。首先，使用操作系统线程来处理并发要求我们使用本质上是操作系统抽象的手段来表示我们的任务。

拥有一个单独的抽象层来表示并发任务让我们自由选择如何处理并发操作。如果我们在 Rust 中创建一个用于并发操作的抽象，比如期货（future）、在 JavaScript 中的承诺（promise）或 Go 中的协程（goroutine），那么由运行时的实现者决定如何处理这些并发任务。

运行时可以简单地将每个并发操作映射到一个操作系统线程，或者使用纤维/绿色线程或状态机来表示任务。编写异步代码的程序员如果底层实现发生变化可能不需要对其代码进行任何更改。

理论上，相同的异步代码可以在没有操作系统的微控制器上用来处理并发操作，只要有相应的运行时即可。

总结
使用操作系统提供的线程来处理并发具有以下优势：

易于理解
易于使用
任务之间的切换相对快速
免费获得并行性
然而，它们也有一些缺点。
将异步操作与线程的概念解耦具有许多好处。首先，使用操作系统线程来处理并发要求我们使用本质上是操作系统抽象的手段来表示我们的任务。

拥有一个单独的抽象层来表示并发任务让我们自由选择如何处理并发操作。如果我们创建一个并发操作的抽象，例如 Rust 中的期货（future）、JavaScript 中的承诺（promise）或 Go 中的协程（goroutine），那么如何处理这些并发任务的决定将取决于运行时的实现者。

运行时可以简单地将每个并发操作映射到一个操作系统线程，也可以使用纤维/绿色线程或状态机来表示任务。编写异步代码的程序员在底层实现发生变更时不一定需要对其代码进行任何修改。

理论上，相同的异步代码可以在没有操作系统的微控制器上用于处理并发操作，只要有适用的运行时即可。

总结
使用操作系统提供的线程来处理并发具有以下优势：

容易理解
易于使用
任务之间的切换相对快速
免费获得并行性
然而，它们也有一些缺点：

操作系统级线程通常具有较大的栈。如果有许多任务同时等待（例如在负载很重的网络服务器中），您会很快耗尽内存。
上下文切换可能会很昂贵，并且由于将所有调度都交给操作系统，您可能会获得不可预测的性能。操作系统需要处理的事务非常多，可能无法像您希望的那样快速切换回您的线程。
它与操作系统抽象紧密耦合。在某些系统上，这可能不是一个可选项。
示例
由于在本书中我们不会花更多时间讨论操作系统线程，因此我们将通过一个简短的示例来展示它们是如何使用的。



ch02/aa-os-threads use std::thread::{self, sleep}; fn main() {     println!("So, we start the program here!");     let t1 = thread::spawn(move || {         sleep(std::time::Duration::from_millis(200));         println!("The long running tasks finish last!");     }); 
    let t2 = thread::spawn(move || {         sleep(std::time::Duration::from_millis(100));         println!("We can chain callbacks...");         let t3 = thread::spawn(move || {             sleep(std::time::Duration::from_millis(50));             println!("...like this!");         });         t3.join().unwrap();     });     println!("The tasks run concurrently!");     t1.join().unwrap();     t2.join().unwrap(); }

在这个示例中，我们简单地启动了几个操作系统线程，并将它们置于休眠状态。休眠本质上与向操作系统调度程序让出控制权的请求相同，希望在经过一段时间后重新调度运行。为了确保我们的主线程不会在子线程有时间运行之前完成并退出（这将导致进程退出），我们在主函数末尾将它们连接（join）在一起。如果我们运行这个示例，就会看到操作的顺序会根据我们将每个线程让给调度程序的时间长短而发生变化：

所以，我们在这里启动程序！
任务并发运行！
我们可以链式回调...
...像这样！
长时间运行的任务最后完成！
因此，尽管使用操作系统线程对许多任务来说非常有效，但我们也概述了一些思考替代方案的良好理由，讨论了它们的局限性和缺点。我们将要讨论的第一个替代方案是我们所称的纤维和绿色线程。

纤维和绿色线程
注意！ 这是一个 M:N 线程的示例，许多任务可以在一个操作系统线程上并发运行。纤维和绿色线程通常被称为“栈满协程”。

“绿色线程”这个名称最初源自 Java 中早期实现的 M:N 线程模型，后来与不同的 M:N 线程实现相关联。您会遇到这个术语的不同变体，例如 Erlang 中使用的“绿色进程”，它们与我们在这里讨论的有所不同。您还会看到一些将绿色线程的定义比我们在此处提供的更为广泛。

在本书中，我们将绿色线程定义为与纤维同义，因此在后续中这两个术语都指代相同的概念。

纤维和绿色线程的实现意味着存在一个运行时和一个负责调度哪个任务（M）在操作系统线程（N）上运行的调度程序。任务的数量远远超过操作系统线程的数量，这样的系统可以只使用一个操作系统线程正常运行。后者的情况通常被称为 M:1 线程。

Goroutines 是栈满协程的一种特定实现，但它有些细微的差别。“协程”这个术语通常暗示它们是协作性的，但 Goroutines 可以被调度程序抢占（至少自版本 1.14 以来），因此在我们在这里提出的类别中，它们处于某种灰色地带。

绿色线程和纤维使用与操作系统相同的机制，为每个任务设置一个栈，保存 CPU 的状态，并通过执行上下文切换从一个任务（线程）跳转到另一个。我们将控制权让给调度程序（这在这样的系统中是运行时的核心部分），然后调度程序继续运行另一个任务。

执行状态存储在每个栈中，因此在这样的解决方案中，不需要 async、await、Future 或 Pin。在许多方面，绿色线程模仿了操作系统如何促进并发，实施它们是一个很好的学习经验。

使用纤维/绿色线程处理并发任务的运行时可以具有很高的灵活性。任务可以在任何时间和执行的任何点被抢占和上下文切换，因此，一个占用 CPU 的长时间运行任务理论上可以被运行时抢占，从而避免由于边缘情况或程序员错误而导致任务阻塞整个系统。

这使得运行时调度程序几乎具备与操作系统调度程序相同的能力，这是使用纤维/绿色线程系统的最大优势之一。

典型流程如下：
您运行一些非阻塞代码。
您对某个外部资源进行阻塞调用。
CPU 跳转到主线程，调度一个不同的线程运行，并跳转到该线程的栈。
您在新线程上运行一些非阻塞代码，直到进行新的阻塞调用或任务完成。
CPU 再次跳转回主线程，调度一个准备好继续执行的新线程，并跳转到该线程。

每个栈都有一个固定的空间

由于纤程（fibers）和绿色线程（green threads）与操作系统线程（OS threads）相似，它们也存在一些相同的缺点。每个任务都被分配了一个固定大小的栈，因此你仍然需要预留比实际使用更多的空间。然而，这些栈可以是可增长的，这意味着一旦栈满了，运行时可以扩展栈的大小。虽然这听起来很简单，但实际上这是一个相当复杂的问题。

我们不能像树一样简单地扩展栈。实际上，需要发生以下两种情况之一：

你分配一个新的连续内存块，并处理栈分布在两个不连续内存段中的情况。
你分配一个更大的新栈（例如，是之前栈大小的两倍），将所有数据移动到新栈中，然后继续执行。
第一种解决方案听起来很简单，因为你可以保留原始栈不变，基本上可以在需要时切换到新栈并继续执行。然而，现代CPU如果能够在一个连续的内存块上工作，由于缓存和它们预测下一条指令将要处理的数据的能力，可以非常快速地工作。将栈分布在两个不连续的内存块中会降低性能。当你有一个循环恰好位于栈边界时，这种情况尤其明显，因此你最终可能会为每次循环迭代进行多达两次的上下文切换。

第二种解决方案通过使栈成为一个连续的内存块来解决第一种解决方案的问题，但它也带来了一些问题。首先，你需要分配一个新栈并将所有数据移动到新栈中。但是，当所有内容都移动到新位置时，指向栈上内容的指针和引用会发生什么？你猜对了：每个指向栈上内容的指针和引用都需要更新，以指向新位置。这既复杂又耗时，但如果你的运行时已经包含垃圾回收器，你已经有了跟踪所有指针和引用的开销，所以这可能比非垃圾回收程序的问题要小。然而，每次栈增长时，都需要垃圾回收器和运行时之间进行大量的集成，因此实现这种运行时可能会变得非常复杂。

其次，你必须考虑如果你有很多长时间运行的任务，这些任务在短时间内需要大量栈空间（例如，如果任务开始时涉及大量递归），但在其余时间主要是I/O绑定的情况。你最终会多次扩展栈，只是为了任务的一个特定部分，并且你必须决定是否接受任务占用比实际需要更多的空间，或者在某个时候将其移回较小的栈。这对你的程序的影响当然会根据你所做的工作类型而有很大差异，但这仍然是你需要注意的事情。

上下文切换

尽管这些纤程/绿色线程与操作系统线程相比是轻量级的，但你仍然需要在每次上下文切换时保存和恢复寄存器。这在大多数情况下可能不会成为问题，但与不需要上下文切换的替代方案相比，它可能会效率较低。上下文切换也可能非常复杂，特别是如果你打算支持许多不同的平台。
调度
当一个纤程（fiber）或绿色线程（green thread）让出控制权给运行时调度器时，调度器可以简单地恢复执行一个新的、已经准备好运行的任务。这意味着你避免了每次让出控制权给调度器时被放入与系统中所有其他任务相同的运行队列的问题。从操作系统的角度来看，你的线程一直在忙于工作，因此操作系统会尽量避免抢占它们。

这种方法的一个意想不到的缺点是，大多数操作系统调度器通过为每个操作系统线程分配一个时间片来确保所有线程都能获得一些运行时间，在这个时间片内，线程可以运行，直到操作系统抢占该线程并在该CPU上调度一个新线程。使用多个操作系统线程的程序可能会被分配更多的时间片，而使用较少操作系统线程的程序则可能被分配较少的时间片。使用M:N线程模型的程序很可能只使用少数几个操作系统线程（在大多数系统上，每个CPU核心一个线程似乎是起点）。因此，根据系统上运行的其他程序，你的程序可能会被分配比使用多个操作系统线程时更少的时间片。然而，考虑到现代CPU上可用的核心数量以及并发系统上的典型工作负载，这种影响应该是最小的。

FFI（外部函数接口）
由于你创建了自己的栈，这些栈在某些条件下可能会增长或缩小，并且可能有一个调度器假设它可以在任何时候抢占正在运行的任务，因此在使用FFI时，你必须采取额外的措施。大多数FFI函数都假设使用操作系统提供的普通C栈，因此从纤程或绿色线程调用FFI函数很可能会出现问题。你需要通知运行时调度器，切换到不同的操作系统线程，并以某种方式通知调度器你已经完成，纤程或绿色线程可以继续执行。这自然会给运行时实现者和进行FFI调用的用户带来额外的开销和复杂性。

优点
对用户来说使用简单。代码看起来与使用操作系统线程时一样。
上下文切换速度相对较快。
与操作系统线程相比，内存使用量较大时问题较小。
你可以完全控制任务的调度方式，并且可以根据需要优先处理它们。
很容易引入抢占机制，这是一个强大的功能。
缺点
当栈空间不足时，栈需要一种增长的方式，这会增加额外的工作和复杂性。
你仍然需要在每次上下文切换时保存CPU状态。
如果你打算支持多个平台和/或CPU架构，正确实现起来会非常复杂。
FFI可能会带来大量开销，并增加意外的复杂性。
基于回调的方法
注意！
这是M:N线程模型的另一个例子。许多任务可以在一个操作系统线程上并发运行。每个任务由一系列回调组成。你可能已经从JavaScript中了解了我们接下来要讨论的内容，我假设大多数人都知道这一点。

基于回调的方法
基于回调的方法的核心思想是保存一组我们稍后想要运行的指令的指针，以及所需的任何状态。在Rust中，这通常是一个闭包（closure）。

在大多数语言中，实现回调相对容易。它们不需要任何上下文切换，也不需要为每个任务预分配内存。

然而，使用回调来表示并发操作要求你从一开始就以完全不同的方式编写程序。将一个使用正常顺序程序流的程序重写为使用回调的程序，需要进行大量的重写工作，反之亦然。

基于回调的并发可能难以推理，并且可能变得非常复杂。大多数JavaScript开发者都熟悉的“回调地狱”这一术语并非偶然。

由于每个子任务必须保存它稍后所需的所有状态，内存使用量将随着任务中回调数量的增加而线性增长。

优点
在大多数语言中易于实现。
不需要上下文切换。
内存开销相对较低（在大多数情况下）。
缺点
内存使用量随着回调数量的增加而线性增长。
程序和代码可能难以推理。
这是一种非常不同的编程方式，它将影响程序的几乎所有方面，因为所有让出操作都需要一个回调。
所有权可能难以推理。因此，在没有垃圾回收器的情况下编写基于回调的程序可能会变得非常困难。
由于所有权规则的复杂性，任务之间共享状态很困难。
调试回调可能很困难。
协程：Promise 和 Future
注意！
这是M:N线程模型的另一个例子。许多任务可以在一个操作系统线程上并发运行。每个任务表示为一个状态机。

JavaScript中的Promise和Rust中的Future是基于相同思想的两种不同实现。

通过这种方式，我们可以编写处理并发操作的程序，几乎就像编写普通的顺序程序一样。

我们的JavaScript程序现在可以写成如下形式：

async function run() {
    await timer(200);
    await timer(100);
    await timer(50);
    console.log("I'm the last one");
}
你可以将run函数视为一个由多个子任务组成的可暂停任务。在每个await点，它将控制权让给调度器（在这种情况下，它是著名的JavaScript事件循环）。一旦其中一个子任务的状态变为fulfilled或rejected，任务就会被调度继续执行下一步。

在使用Rust时，你可以看到类似的转换发生在函数签名中，当你编写如下代码时：

async fn run() -> () { … }
该函数包装了返回对象，并且不是返回类型()，而是返回一个输出类型为()的Future：

Fn run() -> impl Future<Output = ()>
从语法上看，Rust的futures 0.1与我们刚刚展示的promise示例非常相似，而我们现在使用的Rust futures与JavaScript中的async/await工作机制有很多共同之处。

这种将看似普通的函数和代码重写为其他形式的方式有很多好处，但也不是没有缺点。

与任何无栈协程实现一样，完全抢占可能难以实现，甚至不可能实现。这些函数必须在特定点让出控制权，与纤程/绿色线程不同，无法在栈帧中间暂停执行。通过在运行时或编译器在每个函数调用处插入抢占点，可以实现某种程度的抢占，但这与能够在任务执行的任何点抢占任务并不相同。

抢占点
抢占点可以被视为插入代码，调用调度器并询问它是否希望抢占任务。这些点可以由编译器或你使用的库在每个新函数调用之前插入。

此外，你需要编译器支持才能充分利用它。具有元编程能力（如宏）的语言可以模拟许多相同的功能，但这仍然不如编译器意识到这些特殊的异步任务时那样无缝。

调试是另一个在实现futures/promises时需要特别注意的领域。由于代码被重写为状态机（或生成器），你将无法获得与普通函数相同的堆栈跟踪。通常，你可以假设函数的调用者在堆栈和程序流中都位于它之前。对于futures和promises，可能是运行时调用了推进状态机的函数，因此可能没有一个好的回溯可以用来查看在调用失败函数之前发生了什么。有一些方法可以解决这个问题，但大多数方法都会带来一些开销。

优点
你可以像平常一样编写代码和建模程序。
不需要上下文切换。
可以以非常高效的内存方式实现。
易于在各种平台上实现。
缺点
完全实现抢占可能很困难，甚至不可能，因为任务无法在栈帧中间停止。
需要编译器支持才能充分利用其优势。
由于非顺序的程序流以及从回溯中获得的信息有限，调试可能会很困难。
总结
你还在这里？太棒了！恭喜你完成了所有这些背景信息的学习。我知道阅读描述抽象和代码的文本可能会让人望而生畏，但我希望你现在明白为什么在书的开头就讨论这些高级主题对我们来说如此有价值。我们很快就会进入示例部分。我保证！

在本章中，我们讨论了如何通过使用操作系统提供的线程以及编程语言或库提供的抽象来建模和处理编程语言中的异步操作。虽然这不是一个详尽的列表，但我们讨论了一些最流行和广泛使用的技术，并讨论了它们的优点和缺点。

我们花了相当多的时间深入探讨了线程、协程、纤程、绿色线程和回调，因此你应该对它们是什么以及它们之间的区别有了很好的了解。


下一章将详细介绍我们如何进行系统调用并创建跨平台抽象，以及像Epoll、Kqueue和IOCP这样的操作系统支持的事件队列到底是什么，为什么它们是你在大多数异步运行时中会遇到的基础。


# 第三章 Understanding OS-Backed Event Queues, SystemCalls, and Cross-Platform Abstractions 理解操作系统支持的事件队列、系统调用和跨平台抽象

在本章中，我们将探讨操作系统支持的事件队列是如何工作的，以及三种不同的操作系统如何以不同的方式处理这一任务。之所以要深入探讨这一点，是因为我所知的大多数异步运行时都使用这种操作系统支持的事件队列作为实现高性能 I/O 的基础部分。在阅读有关异步代码如何真正工作的内容时，你很可能会经常听到对这些内容的引用。

基于我们在本章讨论的技术的事件队列被用于许多流行的库中，例如：

mio（https://github.com/tokio-rs/mio），这是 Tokio 等流行运行时的关键部分。
polling（https://github.com/smol-rs/polling），这是 Smol 和 async-std 中使用的事件队列。
libuv（https://libuv.org/），这是用于创建 Node.js（一个 JavaScript 运行时）和 Julia 编程语言中使用的事件队列的库。
C# 用于其异步网络调用。
Boost.Asio，这是一个用于 C++ 的异步网络 I/O 库。
我们与主机操作系统的所有交互都是通过系统调用（syscalls）完成的。要在 Rust 中进行系统调用，我们需要知道如何使用 Rust 的外部函数接口（FFI）。

除了了解如何使用 FFI 和进行系统调用外，我们还需要讨论跨平台抽象。在创建事件队列时，无论是自己创建还是使用库，你都会注意到，如果你只对例如 Windows 上的 IOCP 如何工作有一个高层次的了解，这些抽象可能会显得有点不直观。这是因为这些抽象需要提供一个 API，涵盖不同操作系统以不同方式处理相同任务的事实。这个过程通常涉及识别平台之间的共同点，并在此基础上构建一个新的抽象。

为了解释 FFI、系统调用和跨平台抽象，我们将通过一个简单的例子来逐步引入这个话题，而不是使用一个复杂且冗长的例子。当我们稍后遇到这些概念时，我们已经对这些主题有了足够的了解，因此我们可以为后续章节中更有趣的例子做好充分准备。

在本章中，我们将讨论以下主要主题：

为什么要使用操作系统支持的事件队列？
基于就绪状态的事件队列
基于完成的事件队列
epoll
kqueue
IOCP
系统调用、FFI 和跨平台抽象
注意：虽然我们在这里没有涵盖，但你应该了解一些流行的、尽管较少使用的替代方案：

wepoll：它在 Windows 上使用特定的 API 并封装了 IOCP，因此它非常类似于 Linux 上的 epoll，而不是常规的 IOCP。这使得在两个不同的技术上创建一个具有相同 API 的抽象层变得更加容易。它被 libuv 和 mio 使用。
io_uring：这是 Linux 上一个相对较新的 API，与 Windows 上的 IOCP 有许多相似之处。
我相信，在你阅读完接下来的两章后，如果你想要了解更多关于这些内容的信息，你会很容易地阅读这些内容。

技术要求：本章不需要你设置任何新的内容，但由于我们正在为三个不同的平台编写一些低级代码，因此如果你想要运行所有示例，你需要访问这些平台。最好的方法是打开你计算机上的配套仓库并导航到 ch03 文件夹。

本章有点特别，因为我们从基础开始构建一些基本的理解，这意味着其中一些内容相当低级，并且需要特定的操作系统和 CPU 系列才能运行。别担心；我选择了最常用和流行的 CPU，所以这应该不是问题，但这是你需要意识到的事情。

机器必须在 Windows 和 Linux 上使用使用 x86-64 指令集的 CPU。Intel 和 AMD 的桌面 CPU 使用这种架构，但如果你在 ARM 处理器上运行 Linux（或 WSL），你可能会遇到一些使用内联汇编的示例的问题。在 macOS 上，书中的示例针对较新的 M 系列芯片，但仓库中也包含针对较旧的基于 Intel 的 Mac 的示例。

不幸的是，一些针对特定平台的示例需要该特定操作系统才能运行。然而，这将是唯一一章你需要访问三个不同平台才能运行所有示例的章节。接下来，我们将创建可以在所有平台上本地运行或使用 Windows Subsystem for Linux（WSL）运行的示例，但为了理解跨平台抽象的基础知识，我们实际上需要创建针对这些不同平台的示例。

行 Linux 示例
如果你没有设置 Linux 机器，你可以在 Rust Playground 上运行 Linux 示例，或者如果你使用的是 Windows 系统，我建议你设置 WSL 并在那里运行代码。你可以在 https://learn.microsoft.com/en-us/windows/wsl/install 找到如何设置 WSL 的说明。
记住，你还需要在 WSL 环境中安装 Rust，因此请按照本书前言部分关于如何在 Linux 上安装 Rust 的说明进行操作。
如果你使用 VS Code 作为编辑器，有一种非常简单的方法可以将你的环境切换到 WSL。按下 Ctrl+Shift+P，然后输入 Reopen folder in WSL。这样，你可以轻松地在 WSL 中打开示例文件夹，并在 Linux 环境中运行代码示例。

为什么要使用操作系统支持的事件队列？
你现在已经知道，我们需要与操作系统紧密合作，以使 I/O 操作尽可能高效。Linux、macOS 和 Windows 等操作系统提供了多种执行 I/O 的方式，包括阻塞和非阻塞。
I/O 操作需要通过操作系统进行，因为它们依赖于操作系统抽象的资源。这可能是磁盘驱动器、网卡或其他外围设备。特别是在网络调用的情况下，我们不仅依赖于自己的硬件，还依赖于可能远离我们自己的资源，这会导致显著的延迟。
在上一章中，我们讨论了编程时处理异步操作的不同方法，虽然它们各不相同，但它们都有一个共同点：在进行系统调用时，它们需要控制何时以及是否应该让出给操作系统调度程序。
实际上，这意味着需要避免通常会让出给操作系统调度程序的系统调用（阻塞调用），而需要使用非阻塞调用。我们还需要一种高效的方式来了解每个调用的状态，以便我们知道何时可以继续执行原本会阻塞的任务。这就是在异步运行时中使用操作系统支持的事件队列的主要原因。
我们将以三种不同的方式处理 I/O 操作为例进行探讨。

阻塞 I/O
当我们要求操作系统执行阻塞操作时，它将挂起发出调用的操作系统线程。然后，它会存储我们在发出调用时的 CPU 状态，并继续执行其他任务。当通过网络接收到数据时，它会再次唤醒我们的线程，恢复 CPU 状态，并让我们继续执行，就像什么都没发生过一样。
对于程序员来说，阻塞操作是最不灵活的，因为我们在每次调用时都会将控制权让给操作系统。最大的优势是，一旦我们等待的事件准备就绪，我们的线程就会被唤醒，从而可以继续执行。如果我们考虑到整个系统在操作系统上运行的情况，这是一个相当高效的解决方案，因为操作系统会为有工作要做的线程分配 CPU 时间以推进任务。然而，如果我们缩小范围，单独查看我们的进程，我们会发现每次进行阻塞调用时，我们都会让一个线程进入睡眠状态，即使我们的进程仍有工作可以做。这让我们面临选择：要么生成新线程来执行工作，要么接受我们必须等待阻塞调用返回。我们稍后会对此进行更详细的讨论。

非阻塞 I/O
与阻塞 I/O 操作不同，操作系统不会挂起发出 I/O 请求的线程，而是给它一个句柄，线程可以使用该句柄询问操作系统事件是否准备就绪。我们称查询状态的过程为轮询（polling）。
非阻塞 I/O 操作为我们程序员提供了更多的自由，但通常这也伴随着责任。如果我们轮询得太频繁，比如在一个循环中，我们将占用大量的 CPU 时间只是为了询问更新状态，这是非常浪费的。如果我们轮询得太少，事件准备就绪和我们采取行动之间会有显著的延迟，从而限制我们的吞吐量。

通过 epoll/kqueue 和 IOCP 进行事件队列
这是前两种方法的混合体。在网络调用的情况下，调用本身是非阻塞的。然而，我们不需要定期轮询句柄，而是可以将该句柄添加到事件队列中，并且我们可以以极小的开销处理数千个句柄。
作为程序员，我们现在有了一个新的选择。我们可以定期查询队列以检查我们添加的事件是否改变了状态，或者我们可以对队列进行阻塞调用，告诉操作系统我们希望当队列中至少有一个事件改变状态时被唤醒，以便等待该特定事件的任务可以继续执行。
这使我们能够在没有更多工作要做且所有任务都在等待事件发生时才将控制权让给操作系统。我们可以自己决定何时发出这样的阻塞调用。

注意：我们不会涵盖诸如 poll 和 select 之类的方法。大多数操作系统都有一些较旧的方法，这些方法在现代异步运行时中并不广泛使用。只需知道，我们还可以进行其他调用，这些调用本质上试图提供与我们刚刚讨论的事件队列相同的灵活性。

基于就绪状态的事件队列
epoll 和 kqueue 被称为基于就绪状态的事件队列，这意味着它们会在某个操作准备好执行时通知你。一个典型的例子是当一个套接字准备好被读取时。
为了了解这在实践中是如何工作的，我们可以看看使用 epoll 或 kqueue 从套接字读取数据时会发生什么：

我们通过调用系统调用 epoll_create 或 kqueue 创建一个事件队列。
我们向操作系统请求一个表示网络套接字的文件描述符。
通过另一个系统调用，我们注册对该套接字的读取事件（Read events）的兴趣。重要的是，我们还需要通知操作系统，当事件在我们第一步创建的事件队列中准备就绪时，我们希望收到通知。
接下来，我们调用 epoll_wait 或 kevent 来等待事件。这将阻塞（挂起）调用它的线程。
当事件准备就绪时，我们的线程被解除阻塞（恢复），并从等待调用中返回，同时返回有关发生事件的数据。
我们对第二步中创建的套接字调用 read。

图 3.1 – epoll 和 kqueue 流程的简化视图
基于完成状态的事件队列
IOCP 是输入/输出完成端口（Input/Output Completion Port）的缩写。这是一种基于完成状态的事件队列。这种类型的队列会在事件完成时通知你。一个典型的例子是当数据被读取到缓冲区时。
以下是这种事件队列的基本流程：

我们通过调用系统调用 CreateIoCompletionPort 创建一个事件队列。
我们创建一个缓冲区，并向操作系统请求一个套接字的句柄。
我们通过另一个系统调用注册对该套接字的读取事件（Read events）的兴趣，但这次我们还传递了在第二步中创建的缓冲区，数据将被读取到这个缓冲区中。
接下来，我们调用 GetQueuedCompletionStatusEx，它将阻塞，直到某个事件完成。
我们的线程被解除阻塞，缓冲区现在填充了我们感兴趣的数据。

IOCP 是输入/输出完成端口（Input/Output Completion Port）的缩写。 这是一种基于完成状态的事件队列。这种类型的队列会在事件完成时通知你。一个典型的例子是当数据被读取到缓冲区时。
以下是这种事件队列的基本流程：

我们通过调用系统调用 CreateIoCompletionPort 创建一个事件队列。
我们创建一个缓冲区，并向操作系统请求一个套接字的句柄。
我们通过另一个系统调用注册对该套接字的读取事件（Read events）的兴趣，但这次我们还传递了在第二步中创建的缓冲区，数据将被读取到这个缓冲区中。
接下来，我们调用 GetQueuedCompletionStatusEx，它将阻塞，直到某个事件完成。
我们的线程被解除阻塞，缓冲区现在填充了我们感兴趣的数据。
图 3.2 – IOCP 流程的简化视图

epoll、kqueue 和 IOCP
epoll 是 Linux 实现事件队列的方式。在功能上，它与 kqueue 有很多共同点。在 Linux 上使用 epoll 相对于其他类似方法（如 select 或 poll）的优势在于，epoll 被设计为能够非常高效地处理大量事件。

kqueue 是 macOS 实现事件队列的方式（起源于 BSD），在 FreeBSD 和 OpenBSD 等操作系统中也有使用。从高层次的功能来看，它在概念上与 epoll 相似，但在实际使用中有所不同。

IOCP 是 Windows 处理这种事件队列的方式。在 Windows 中，完成端口会在事件完成时通知你。这听起来可能像是一个微小的区别，但实际上并非如此。这在编写库时尤为明显，因为抽象化这两种方式意味着你必须将 IOCP 建模为基于就绪状态（readiness-based）或将 epoll/kqueue 建模为基于完成状态（completion-based）。向操作系统借出缓冲区也带来了一些挑战，因为在等待操作返回时，保持缓冲区不被修改非常重要。

平台	事件队列类型
Windows	IOCP
Linux	epoll
macOS	kqueue
类型	基于完成状态
表 3.1 – 不同平台和事件队列

跨平台事件队列
在创建跨平台事件队列时，你必须处理这样一个事实：你需要创建一个统一的 API，无论它是用于 Windows（IOCP）、macOS（kqueue）还是 Linux（epoll）。最明显的区别是，IOCP 是基于完成状态的，而 kqueue 和 epoll 是基于就绪状态的。

这种根本性的区别意味着你必须做出选择：

你可以创建一个抽象层，将 kqueue 和 epoll 视为基于完成状态的队列，或者
你可以创建一个抽象层，将 IOCP 视为基于就绪状态的队列。
根据我的个人经验，创建一个模仿基于完成状态的队列的抽象层，并在幕后处理 kqueue 和 epoll 是基于就绪状态的事实，要比反过来容易得多。正如我之前提到的，使用 wepoll 是在 Windows 上创建基于就绪状态的队列的一种方式。这将大大简化创建此类 API 的过程，但我们现在暂时不讨论这一点，因为它不太为人所知，也不是微软官方文档中推荐的方法。

由于 IOCP 是基于完成状态的，它需要一个缓冲区来读取数据，因为它会在数据读取到该缓冲区时返回。而 kqueue 和 epoll 则不需要这样做。它们只会在你可以将数据读取到缓冲区而不会阻塞时返回。

通过要求用户为我们的 API 提供他们首选大小的缓冲区，我们让用户控制他们如何管理内存。用户定义缓冲区的大小，并在使用 IOCP 时控制传递给操作系统的内存的所有方面。



图 3.2 – IOCP 流的简化视图
epoll、kqueue 和 IOCP
epoll 是 Linux 实现事件队列的一种方式。在功能上，它与 kqueue 有很多相似之处。使用 epoll 的优势在于，它被设计得非常高效，可以处理大量事件，而其他类似的方法，例如 select 或 poll，效率相对较低。

kqueue 是 macOS 实现事件队列的一种方式（起源于 BSD），在 FreeBSD 和 OpenBSD 等操作系统中使用。从高层功能上看，它的概念�� epoll 类似，但在实际使用中有所不同。

IOCP 是 Windows 处理此类型事件队列的方式。在 Windows 中，完成端口会在事件完成时通知您。虽然这听起来可能是一个小差异，但实际上并非如此。当您想编写一个库时，这一点尤其明显，因为对这两者的抽象意味着您要么必须将 IOCP 模型化为基于准备的，要么将 epoll/kqueue 模型化为基于完成的。

将缓冲区借给操作系统也带来一些挑战，因为在等待操作返回期间，让这个缓冲区保持不变是非常重要的。

平台	Windows	Linux	macOS
IOCP	epoll	kqueue	
基于完成	基于准备	基于准备	
表 3.1 – 不同平台和事件队列

跨平台事件队列
在创建跨平台事件队列时，您必须处理创建一个统一 API 的事实，该 API 在 Windows（IOCP）、macOS（kqueue）或 Linux（epoll）上使用时都保持一致。最明显的区别是 IOCP 是基于完成的，而 kqueue 和 epoll 是基于准备的。

这种根本的区别意味着您必须做出选择：

您可以创建一个将 kqueue 和 epoll 视为基于完成的队列的抽象，或者
您可以创建一个将 IOCP 视为基于准备的队列的抽象。
根据我的个人经验，创建一个模拟基于完成队列的抽象并在后台处理 kqueue 和 epoll 是基于准备的事实，这要比反过来要容易得多。正如我之前提到的，使用 wepoll 是在 Windows 上创建基于准备的队列的一种方法。这将极大简化创建这样的 API，但我们暂时不讨论这个，因为它较少为人所知，也不是微软官方文档中记录的方法。


由于 IOCP 是基于完成状态的，它需要一个缓冲区来读取数据，因为它会在数据读取到该缓冲区时返回。而 kqueue 和 epoll 则不需要这样做。它们只会在你可以将数据读取到缓冲区而不会阻塞时返回。
通过要求用户为我们的 API 提供他们首选大小的缓冲区，我们让用户控制他们如何管理内存。用户定义缓冲区的大小，并在使用 IOCP 时控制传递给操作系统的内存的所有方面。

在 epoll 和 kqueue 的情况下，对于这样的 API，你可以简单地为用户调用 read 并填充相同的缓冲区，从而让用户感觉 API 是基于完成状态的。
如果你想提供一个基于就绪状态的 API，你必须在 Windows 上进行 I/O 操作时制造一种假象，即有两个独立的操作：首先，请求在套接字上的数据准备好读取时通知你，然后实际读取数据。虽然这是可能的，但你很可能会发现自己需要创建一个非常复杂的 API，或者由于需要中间缓冲区来维持基于就绪状态的 API 的假象，而在 Windows 平台上接受一些效率上的损失。

我们将把事件队列的话题留到后面，当我们创建一个简单的示例来展示它们究竟如何工作时再讨论。在此之前，我们需要真正熟悉 FFI（外部函数接口）和系统调用，我们将通过在三个不同平台上编写一个系统调用的示例来实现这一点。
我们还将利用这个机会讨论抽象层次，以及如何创建一个在三个不同平台上工作的统一 API。

系统调用、FFI 和跨平台抽象
我们将为三种架构实现一个非常基本的系统调用：BSD/macOS、Linux 和 Windows。我们还将看到如何在三个抽象层次上实现这一点。
我们将实现的系统调用是用于向标准输出（stdout）写入内容的调用，因为这是一个非常常见的操作，而且了解它的实际工作原理非常有趣。

我们将从最低层次的抽象开始，逐步构建对系统调用的理解。

最低层次的抽象
最低层次的抽象是编写通常称为“原始”系统调用的代码。原始系统调用绕过了操作系统提供的用于进行系统调用的库，而是依赖于操作系统具有稳定的系统调用 ABI（应用程序二进制接口）。稳定的系统调用 ABI 意味着它保证如果你将正确的数据放入某些寄存器并调用一个特定的 CPU 指令将控制权传递给操作系统，它总是会执行相同的操作。

要进行原始系统调用，我们需要编写一些内联汇编代码，但不用担心。尽管我们在这里突然引入它，但我们将逐行解释，并在第 5 章中更详细地介绍内联汇编，以便你熟悉它。
在这个抽象层次上，我们需要为 BSD/macOS、Linux 和 Windows 编写不同的代码。如果操作系统运行在不同的 CPU 架构上，我们还需要编写不同的代码。

* Raw syscall on Linux
在 Linux 和 macOS 上，我们想要调用的系统调用名为 write。这两个系统都基于文件描述符的概念运行，并且当你启动一个进程时，标准输出（stdout）已经存在。
如果你的机器上没有运行 Linux，你有几种选择来运行这个示例。你可以将代码复制并粘贴到 Rust Playground 中，或者你可以使用 Windows 上的 WSL（Windows Subsystem for Linux）来运行它。

正如在介绍中提到的，我将在每个示例的开头列出你需要跳转到的示例，你可以通过编写 cargo run 来运行该示例。源代码本身始终位于 example 文件夹中的 src/main.rs 文件中。

我们要做的第一件事是引入标准库模块，该模块使我们能够访问 asm! 宏。
仓库参考：ch03/a-raw-syscall

use std::arch::asm;
接下来是编写我们的系统调用函数：

#[inline(never)]
fn syscall(message: String) {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    unsafe {
        asm!(
            "mov rax, 1",
            "mov rdi, 1",
            "syscall",
            in("rsi") msg_ptr,
            in("rdx") len,
            out("rax") _,
            out("rdi") _,
            lateout("rsi") _,
            lateout("rdx") _
        );
    }
}
我们将逐行解释这个函数。接下来的函数会非常相似，所以我们只需要详细解释一次。
首先，我们有一个名为 #[inline(never)] 的属性，它告诉编译器我们永远不希望这个函数在优化过程中被内联。内联是指编译器省略函数调用，而是直接复制函数体。在这种情况下，我们不希望这种情况发生。

接下来是我们的函数调用。函数中的前两行只是获取存储文本的内存位置的原始指针以及文本缓冲区的长度。
下一行是一个 unsafe 块，因为在 Rust 中无法安全地调用这样的汇编代码。

汇编的第一行将值 1 放入 rax 寄存器。当 CPU 稍后捕获我们的调用并将控制权传递给操作系统时，内核知道 rax 中的值为 1 意味着我们想要进行 write 操作。

* Raw syscall on macOS

现在，由于我们使用了特定于 CPU 架构的指令，因此根据你运行的是带有 Intel CPU 的旧款 Mac 还是带有基于 Arm 64 架构 CPU 的新款 Mac，我们需要不同的函数。我们只展示适用于使用 ARM64 架构的新 M 系列芯片的代码，但不用担心，如果你克隆了 Github 仓库，你会在那里找到适用于两种版本 Mac 的代码。

由于只有一些微小的变化，我将在这里展示整个示例，并只讲解其中的差异。
请记住，你需要在带有 macOS 和 M 系列芯片的机器上运行此代码。你无法在 Rust Playground 中尝试此代码。

ch03/a-raw-syscall
use std::arch::asm;

fn main() {
    let message = "Hello world from raw syscall!\n";
    let message = String::from(message);
    syscall(message);
}

#[inline(never)]
fn syscall(message: String) {
    let ptr = message.as_ptr();
    let len = message.len();
    unsafe {
        asm!(
            "mov x16, 4",
            "mov x0, 1",
            "svc 0",
            in("x1") ptr,
            in("x2") len,
            out("x16") _,
            out("x0") _,
            lateout("x1") _,
            lateout("x2") _
        );
    }
}
除了寄存器命名不同之外，这与我们为 Linux 编写的代码没有太大区别，唯一的例外是在 macOS 上，write 操作的代码是 4，而不是 Linux 上的 1。此外，发出软件中断的 CPU 指令是 svc 0，而不是 syscall。

再次强调，如果你在 macOS 上运行此代码，你将在控制台上看到以下输出：

Hello world from raw syscall!
关于 Windows 上的原始系统调用
这是一个很好的机会来解释为什么编写原始系统调用（就像我们刚刚做的那样）是一个坏主意，如果你希望你的程序或库跨平台工作的话。

你看，如果你希望你的代码在未来也能正常工作，你必须担心操作系统提供的保证。例如，Linux 保证写入 rax 寄存器的值 1 将始终引用 write，但 Linux 运行在许多平台上，并不是每个人都使用相同的 CPU 架构。我们在 macOS 上也遇到了同样的问题，它最近从使用基于 Intel 的 x86_64 架构转变为基于 ARM 64 的架构。


Windows 在涉及此类低级内部机制时绝对不提供任何保证。Windows 已经多次更改其内部机制，并且没有提供关于此事的官方文档。我们唯一有的是在互联网上找到的反编译表格，但这些并不是一个可靠的解决方案，因为在你下次运行 Windows 更新时，原本是 write 系统调用的内容可能会被更改为 delete 系统调用。即使这种情况不太可能发生，你也没有任何保证，这反过来使得你无法向你的程序用户保证它在未来能够正常工作。

因此，虽然理论上原始系统调用是有效的，并且熟悉它们是有益的，但它们主要作为一个例子，说明为什么我们更愿意链接到不同操作系统为我们提供的库来进行系统调用。下一节将展示我们如何做到这一点。

下一层抽象
下一层抽象是使用所有三个操作系统为我们提供的 API。我们很快就会发现，这种抽象帮助我们减少了一些代码。在这个特定的例子中，Linux 和 macOS 上的系统调用是相同的，所以我们只需要担心是否在 Windows 上运行。我们可以通过使用 #[cfg(target_family = "windows")] 和 #[cfg(target_family = "unix")] 条件编译标志来区分平台。你将在仓库中的示例中看到这些标志的使用。

我们的 main 函数将看起来与之前相同：

ch03/b-normal-syscall
use std::io;

fn main() {
    let message = "Hello world from syscall!\n";
    let message = String::from(message);
    syscall(message).unwrap();
}
唯一的区别是我们不再引入 asm 模块，而是引入 io 模块。

在 Linux 和 macOS 中使用操作系统提供的 API
你可以在 Rust Playground 中直接运行此代码，因为它运行在 Linux 上，或者你可以在使用 WSL 的 Linux 机器上或 macOS 上本地运行它：

ch03/b-normal-syscall
#[cfg(target_family = "unix")]
#[link(name = "c")]
extern "C" {
    fn write(fd: u32, buf: *const u8, count: usize) -> i32;
}

fn syscall(message: String) -> io::Result<()> {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    let res = unsafe { write(1, msg_ptr, len) };
    if res == -1 {
        return Err(io::Error::last_os_error());
    }
    Ok(())
}
让我们一步一步地了解这些步骤。知道如何正确地进行系统调用将在本书的后续部分对我们非常有用。

#[link(name = "c")]

每个 Linux（和 macOS）系统都附带一个版本的 libc，这是一个用于与操作系统通信的 C 库。拥有 libc 及其一致的 API 使我们能够以相同的方式编程，而无需担心底层平台架构。内核开发者也可以对底层 ABI 进行更改，而不会破坏所有人的程序。这个标志告诉编译器链接到系统上的 “c” 库。

接下来是我们想要调用的链接库中的函数定义：

extern "C" {
    fn write(fd: u32, buf: *const u8, count: usize);
}
extern "C"（有时不写 “C”，因为如果没有指定，“C” 是默认的）意味着我们希望在调用我们链接的 “C” 库中的 write 函数时使用 “C” 调用约定。这个函数需要与我们要链接的库中的函数具有完全相同的名称。参数不必具有相同的名称，但它们必须按相同的顺序排列。最好将它们命名为与你要链接的库中相同的名称。

在这里，我们使用 Rust 的 FFI（外部函数接口），所以当你读到使用 FFI 调用外部函数时，这正是我们在这里所做的。

write 函数接受一个文件描述符 fd，在这种情况下是标准输出的句柄。此外，它期望我们提供一个指向 u8 数组的指针 buf 以及该缓冲区的长度 count。

调用约定
这是我们第一次遇到这个术语，所以我会简要解释一下，尽管我们会在本书的后面深入探讨这个话题。

调用约定定义了如何进行函数调用，并且会指定以下内容：

参数如何传递给函数
函数在开始时预期存储哪些寄存器，并在返回前恢复
函数如何返回其结果
如何设置堆栈（我们稍后会回到这一点）
因此，在调用外部函数之前，你需要指定使用哪种调用约定，因为如果我们不告诉编译器，编译器无法知道。C 调用约定是目前最常见的一种。

接下来，我们将对链接函数的调用包装在一个普通的 Rust 函数中。

ch03/b-normal-syscall
#[cfg(target_family = "unix")]
fn syscall(message: String) -> io::Result<()> {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    let res = unsafe { write(1, msg_ptr, len) };
    if res == -1 {
        return Err(io::Error::last_os_error());
    }
    Ok(())
}
你现在可能已经熟悉前两行了，因为它们与我们编写的原始系统调用示例相同。我们获取存储文本的缓冲区的指针以及该缓冲区的长度。

接下来是对 libc 中的 write 函数的调用，这需要包装在一个 unsafe 块中，因为 Rust 在调用外部函数时无法保证安全性。

你可能会想知道我们如何知道值 1 指的是标准输出的文件句柄。在从 Rust 编写系统调用时，你会经常遇到这种情况。通常，常量在 C 头文件中定义，因此我们需要手动查找这些定义。在 UNIX 系统中，1 始终是标准输出的文件句柄，所以很容易记住。

注意
包装 libc 函数并提供这些常量正是 libc crate（https://github.com/rust-lang/libc）为我们提供的。大多数时候，你可以使用它，而不是像我们在这里那样手动链接和定义函数。

最后，我们有错误处理，你在使用 FFI 时会经常看到这一点。C 函数通常使用特定的整数来指示函数调用是否成功。在这个 write 调用的情况下，函数将返回写入的字节数，或者如果出现错误，则返回值 -1。你可以通过阅读 Linux 的 man 页面（https://man7.org/linux/man-pages/index.html）轻松找到这些信息。

如果出现错误，我们使用 Rust 标准库中的内置函数查询操作系统报告给此进程的最后一个错误，并将其转换为 Rust 的 io::Error 类型。

如果你使用 cargo run 运行此函数，你将看到以下输出：

Hello world from syscall


* using windows api
在 Windows 上，事情的工作方式有些不同。虽然 UNIX 将几乎所有东西都建模为你与之交互的“文件”，但 Windows 使用了其他抽象。在 Windows 上，你会获得一个句柄，该句柄代表你可以以特定方式与之交互的某个对象，具体取决于你拥有的句柄类型。

我们将使用与之前相同的 main 函数，但需要链接到 Windows API 中的不同函数，并对我们的 syscall 函数进行一些更改。

ch03/b-normal-syscall
#[link(name = "kernel32")]
extern "system" {
    fn GetStdHandle(nStdHandle: i32) -> i32;
    fn WriteConsoleW(
        hConsoleOutput: i32,
        lpBuffer: *const u16,
        numberOfCharsToWrite: u32,
        lpNumberOfCharsWritten: *mut u32,
        lpReserved: *const std::ffi::c_void,
    ) -> i32;
}
你首先注意到的是，我们不再链接到 “C” 库，而是链接到 kernel32 库。接下来的变化是使用了 system 调用约定。这个调用约定有点特殊。你看，Windows 根据你是为 32 位 x86 Windows 版本还是 64 位 x86_64 Windows 版本编写代码，使用不同的调用约定。在 x86_64 上运行的较新 Windows 版本使用 “C” 调用约定，因此如果你有一个较新的系统，你可以尝试更改它，看看它是否仍然有效。“指定 system”让编译器根据系统确定要使用的正确调用约定。

我们在 Windows 中链接了两个不同的系统调用：

GetStdHandle：获取对标准设备（如标准输出）的引用。
WriteConsoleW：WriteConsole 有两种类型。WriteConsoleW 接受 Unicode 文本，而 WriteConsoleA 接受 ANSI 编码的文本。我们在程序中使用的是接受 Unicode 文本的那个。
现在，如果你只写英文文本，ANSI 编码的文本可以正常工作，但一旦你写其他语言的文本，你可能需要使用 ANSI 中无法表示但在 Unicode 中可以表示的特殊字符。如果你混淆了它们，你的程序将不会按预期工作。

接下来是我们的新 syscall 函数：

ch03/b-normal-syscall
fn syscall(message: String) -> io::Result<()> {
    let msg: Vec<u16> = message.encode_utf16().collect();
    let msg_ptr = msg.as_ptr();
    let len = msg.len() as u32;
    let mut output: u32 = 0;
    let handle = unsafe { GetStdHandle(-11) };
    if handle == -1 {
        return Err(io::Error::last_os_error());
    }
    let res = unsafe {
        WriteConsoleW(
            handle,
            msg_ptr,
            len,
            &mut output,
            std::ptr::null()
        )
    };
    if res == 0 {
        return Err(io::Error::last_os_error());
    }
    Ok(())
}
我们做的第一件事是将文本转换为 Windows 使用的 utf-16 编码文本。幸运的是，Rust 有一个内置函数可以将我们的 utf-8 编码文本转换为 utf-16 代码点。encode_utf16 返回一个 u16 代码点的迭代器，我们可以将其收集到一个 Vec 中。


接下来的两行现在应该很熟悉了。我们获取存储文本的指针以及文本的字节长度。接下来，我们调用 GetStdHandle 并传入值 -11。我们需要为不同的标准设备传入的值在 GetStdHandle 文档中有详细描述，文档地址为：https://learn.microsoft.com/en-us/windows/console/getstdhandle。这很方便，因为我们不需要翻阅 C 头文件来查找所有需要的常量值。

所有函数的返回代码也有详细的文档记录，因此我们在这里处理潜在错误的方式与处理 Linux/macOS 系统调用的方式相同。

最后，我们调用 WriteConsoleW 函数。这并没有什么特别复杂的地方，你会注意到它与我们在 Linux 上使用的 write 系统调用有相似之处。一个区别是输出不是从函数返回的，而是写入我们以指针形式传入的输出变量地址。

注意
现在你已经看到了我们如何创建跨平台的系统调用，你可能也会理解为什么我们没有在本书中为每个示例都包含跨平台代码。如果这样做，这本书会变得非常长，而且并不明显所有这些额外的信息是否真的有助于我们理解关键概念。

最高层次的抽象
这很简单，但我想为了完整性而添加这一点。Rust 标准库为我们封装了对底层操作系统 API 的调用，因此我们不需要关心要调用哪些系统调用。

fn main() {
    println!("Hello world from the standard library");
}
恭喜！你现在已经使用三种抽象层次编写了相同的系统调用。你现在知道 FFI 是什么样子了，你已经看到了一些内联汇编（我们稍后会详细讨论），并且你已经正确地调用了系统调用来将内容打印到控制台。你还看到了标准库试图通过为不同平台封装这些调用来解决的问题之一，这样我们就不需要知道这些系统调用来将内容打印到控制台。

总结
在本章中，我们介绍了操作系统支持的事件队列是什么，并对其工作原理进行了高层次的概述。我们还讨论了 epoll、kqueue 和 IOCP 的定义特征，并重点介绍了它们之间的区别。

 

在本章的后半部分，我们介绍了一些系统调用的示例。我们讨论了原始系统调用和“普通”系统调用，以便你知道它们是什么，并看到了两者的示例。我们还借此机会讨论了抽象层次以及在我们能够使用良好抽象时依赖它们的好处。

作为系统调用的一部分，你还初步了解了 Rust 的 FFI（外部函数接口）。

最后，我们创建了一个跨平台的抽象。你也看到了创建一个适用于多个操作系统的统一 API 所带来的一些挑战。

下一章将带你通过一个使用 epoll 创建简单事件队列的示例，这样你就可以确切地看到它在实践中是如何工作的。在代码仓库中，你还可以找到适用于 Windows 和 macOS 的相同示例，因此如果你曾想为这两个平台中的任何一个实现事件队列，你都可以参考这些示例。



# Part 2:Event Queues and Green Threads 第二部分：事件队列和绿色线程


在这一部分，我们将展示两个示例。第一个示例演示了如何使用 epoll 创建一个事件队列。我们将设计一个与 mio 所使用的 API 非常相似的 API，以便我们能够掌握 mio 和 epoll 的基础知识。第二个示例展示了如何使用纤程/绿色线程，类似于 Go 所采用的方法。这种方法是 Rust 使用 futures 和 async/await 进行异步编程的流行替代方案之一。Rust 在 1.0 版本之前也使用过绿色线程，因此它是 Rust 异步历史的一部分。在整个探索过程中，我们将深入探讨一些基本的编程概念，如指令集架构（ISA）、应用二进制接口（ABI）、调用约定、栈，并简要涉及汇编编程。这一部分包括以下章节：

第 4 章：创建你自己的事件队列
在本章中，我们将使用 epoll 创建一个简单的事件队列。我们将从 mio（https://github.com/tokio-rs/mio）中汲取灵感，这是一个用 Rust 编写的低级 I/O 库，支撑了 Rust 异步生态系统的很大一部分。从 mio 中汲取灵感还有一个额外的好处，那就是如果你希望探索一个真正的生产级库是如何工作的，可以更容易地深入研究它们的代码库。

在本章结束时，你应该能够理解以下内容：

阻塞和非阻塞 I/O 的区别
如何使用 epoll 创建自己的事件队列
跨平台事件队列库（如 mio）的源代码
如果我们希望程序或库能够在不同平台上运行，为什么需要在 epoll、kqueue 和 IOCP 之上构建一个抽象层
我们将本章分为以下几个部分：

epoll 的设计与介绍
ffi 模块
Poll 模块
主程序
技术要求
本章重点介绍 epoll，它是特定于 Linux 的。不幸的是，epoll 不是可移植操作系统接口（POSIX）标准的一部分，因此这个示例需要你在 Linux 上运行，无法在 macOS、BSD 或 Windows 操作系统上运行。

如果你已经在运行 Linux 的机器上，那么你已经准备好了，可以直接运行示例，无需其他步骤。

如果你在 Windows 上，我建议你设置 WSL（https://learn.microsoft.com/en-us/windows/wsl/install），如果还没有的话，并在 WSL 上运行的 Linux 操作系统中安装 Rust。

如果你使用的是 Mac，你可以创建一个运行 Linux 的虚拟机（VM），例如使用基于 QEMU 的 UTM 应用程序（https://mac.getutm.app/）或任何其他用于管理 Mac 上虚拟机的解决方案。


最后一个选项是租用一台 Linux 服务器（甚至有一些提供商提供免费层），安装 Rust，然后在控制台中使用诸如 Vim 或 Emacs 的编辑器，或者通过 SSH 使用 VS Code 在远程机器上进行开发（https://code.visualstudio.com/docs/remote/ssh）。我个人对 Linode 的服务（https://www.linode.com/）有很好的体验，但市场上还有很多其他选择。

理论上，可以在 Rust Playground 上运行这些示例，但由于我们需要一个延迟服务器，因此必须使用一个接受普通 HTTP 请求（而不是 HTTPS）的远程延迟服务器服务，并修改代码，使所有模块都在一个文件中。这在紧急情况下是可行的，但并不推荐。

延迟服务器
这个示例依赖于对一个服务器的调用，该服务器会延迟响应一段可配置的时间。在代码库的根文件夹中，有一个名为 delayserver 的项目。

你可以通过在一个单独的控制台窗口中进入该文件夹并输入 cargo run 来设置服务器。只需让服务器在一个单独的、打开的终端窗口中运行，因为我们将在示例中使用它。

delayserver 程序是跨平台的，因此无需任何修改即可在 Rust 支持的所有平台上运行。如果你在 Windows 上运行 WSL，我建议也在 WSL 中运行 delayserver 程序。根据你的设置，你可能可以在 Windows 控制台中运行服务器，并在 WSL 中运行示例时仍然能够访问它。但请注意，它可能无法开箱即用。

服务器默认监听 8080 端口，示例中假设使用此端口。你可以在启动服务器之前在 delayserver 代码中更改监听端口，但请记住在示例代码中进行相同的修改。

delayserver 的实际代码不到 30 行，因此如果你想了解服务器的功能，浏览代码只需几分钟。

设计与 epoll 介绍
好了，本章将围绕代码库中 ch04/a-epoll 下的一个主要示例展开。我们将从设计示例开始。

正如我在本章开头提到的，我们将从 mio 中汲取灵感。这有一个很大的优点和一个缺点。优点是我们可以轻松了解 mio 的设计方式，如果你想学习比本示例中更多的内容，可以更容易地深入研究其代码库。缺点是我们引入了一个过于厚重的抽象层，覆盖了 epoll，包括一些非常特定于 mio 的设计决策。

我认为优点大于缺点，原因很简单：如果你想实现一个生产质量的事件循环，你可能会想看看现有的实现，如果你想深入了解 Rust 异步编程的构建块，也是如此。在 Rust 中，mio 是支撑大部分异步生态系统的重要库之一，因此对它有一点熟悉是一个额外的收获。

需要注意的是，mio 是一个跨平台库，它在 epoll、kqueue 和 IOCP（通过 Wepoll，如我们在第 3 章中所述）之上创建了一个抽象层。不仅如此，mio 还支持 iOS 和 Android，未来可能还会支持其他平台。因此，如果你将其与仅支持一个平台所能实现的功能进行比较，那么为这么多不同系统统一 API 必然会带来一些妥协。

MIO
mio 自称为“一个快速、低级的 Rust I/O 库，专注于非阻塞 API 和事件通知，旨在以尽可能少的开销在操作系统抽象之上构建高性能 I/O 应用程序。”

mio 驱动了 Tokio 中的事件队列，Tokio 是 Rust 中最受欢迎和广泛使用的异步运行时之一。这意味着 mio 正在为诸如 Actix Web（https://actix.rs/）、Warp（https://github.com/seanmonstar/warp）和 Rocket（https://rocket.rs/）等流行框架驱动 I/O。

在本示例中，我们将以 mio 0.8.8 版本作为设计灵感。API 在过去已经发生了变化，未来可能还会发生变化，但我们在这里介绍的 API 部分自 2019 年以来一直保持稳定，因此在不久的将来不太可能发生重大变化。

与所有跨平台抽象一样，通常有必要选择最小公分母。一些选择将限制一个或多个平台上的灵活性和效率，以追求一个适用于所有平台的统一 API。我们将在本章中讨论其中的一些选择。

在我们继续之前，让我们创建一个空白项目并为其命名。我们将它称为 a-epoll，但你当然需要用你选择的名称替换它。

进入文件夹并输入 cargo init 命令。

在本示例中，我们将项目分为几个模块，并将代码拆分为以下文件：

src
  |-- ffi.rs
  |-- main.rs
  |-- poll.rs
它们的描述如下：

ffi.rs：此模块将包含与我们需要与主机操作系统通信的系统调用相关的代码。
main.rs：这是示例程序本身。
poll.rs：此模块包含主要抽象，即 epoll 之上的一个薄层。
接下来，在 src 文件夹中创建上述四个文件。在 main.rs 中，我们还需要声明这些模块。


a-epoll/src/main.rs
mod ffi;
mod poll;
现在我们已经设置好了项目，我们可以开始讨论如何设计我们将要使用的 API。主要的抽象在 poll.rs 中，所以请打开该文件。

让我们从定义我们需要的结构和函数开始。当我们把它们放在面前时，讨论起来会更容易：

a-epoll/src/poll.rs
use std::{io::{self, Result}, net::TcpStream, os::fd::AsRawFd};
use crate::ffi;

type Events = Vec<ffi::Event>;

pub struct Poll {
  registry: Registry,
}

impl Poll {
  pub fn new() -> Result<Self> {
    todo!()
  }

  pub fn registry(&self) -> &Registry {
    &self.registry
  }

  pub fn poll(&mut self, events: &mut Events, timeout: Option<i32>) -> Result<()> {
    todo!()
  }
}

pub struct Registry {
  raw_fd: i32,
}

impl Registry {
  pub fn register(&self, source: &TcpStream, token: usize, interests: i32) -> Result<()> {
    todo!()
  }
}

impl Drop for Registry {
  fn drop(&mut self) {
    todo!()
  }
}
我们现在已经将所有实现替换为 todo!()。这个宏将允许我们编译程序，尽管我们还没有实现函数体。如果我们的执行过程到达 todo!()，它将会 panic。

你首先会注意到的是，除了标准库中的一些类型外，我们还将 ffi 模块引入作用域。

我们还将使用 std::io::Result 类型作为我们自己的 Result 类型。这很方便，因为大多数错误将来自于我们对操作系统的调用，而操作系统错误可以映射到 io::Error 类型。

epoll 有两个主要的抽象。一个是一个名为 Poll 的结构体，另一个是 Registry。这些函数的名称和功能与 mio 中的相同。命名这样的抽象出人意料地困难，这两个结构体本可以有其他名称，但让我们依赖这样一个事实：有人在我们之前已经花时间思考过这个问题，并决定在我们的示例中使用这些名称。

Poll 是一个表示事件队列本身的结构体。它有几个方法：

new：创建一个新的事件队列。
registry：返回一个注册表的引用，我们可以用它来注册我们感兴趣的事件通知。
poll：阻塞调用它的线程，直到有事件准备好或超时，以先发生者为准。
Registry 是另一个重要的部分。虽然 Poll 表示事件队列，但 Registry 是一个句柄，允许我们注册对新事件的兴趣。

Registry 只有一个方法：register。同样，我们模仿了 mio 使用的 API（https://docs.rs/mio/0.8.8/mio/struct.Registry.html），而不是接受一个预定义的方法列表来注册不同的兴趣，我们接受一个 interests 参数，它将指示我们希望事件队列跟踪哪些类型的事件。

还有一点需要注意的是，我们不会为所有源使用泛型类型。我们只会为 TcpStream 实现这一点，尽管我们可以用事件队列跟踪许多东西。特别是当我们希望使其跨平台时，根据你想要支持的平台，我们可能希望跟踪许多类型的事件源。

mio 通过让 Registry::register 接受一个实现了 mio 定义的 Source trait 的对象来解决这个问题。只要你为源实现了这个 trait，你就可以使用事件队列来跟踪它上面的事件。

在下面的伪代码中，你将了解我们计划如何使用这个 API：


let queue = Poll::new().unwrap();
let id = 1;
// 注册对 TcpStream 上的事件的兴趣
queue.registry().register(&stream, id, ...).unwrap();
let mut events = Vec::with_capacity(1);
// 这将阻塞当前线程
queue.poll(&mut events, None).unwrap();
// ...数据在其中一个被跟踪的流上准备好了
你可能会想知道为什么我们需要 Registry 结构体。要回答这个问题，我们需要记住 mio 是对 epoll、kqueue 和 IOCP 的抽象。它通过让 Registry 包装一个 Selector 对象来实现这一点。Selector 对象是条件编译的，因此每个平台都有自己的 Selector 实现，对应于相关的系统调用，以使 IOCP、kqueue 和 epoll 做同样的事情。

Registry 实现了一个重要的方法，我们不会在我们的示例中实现，这个方法叫做 try_clone。我们不实现它的原因是我们不需要它来理解像这样的事件循环是如何工作的，而且我们希望保持示例简单易懂。然而，这个方法对于理解为什么注册事件和队列本身的责任是分开的很重要。

重要说明
通过将注册兴趣的责任转移到像这样的单独结构体，用户可以调用 Registry::try_clone 来获取一个拥有的 Registry 实例。这个实例可以传递给其他线程，或者通过 Arc<Registry> 共享，允许多个线程向同一个 Poll 实例注册兴趣，即使 Poll 在等待新事件发生时阻塞了另一个线程。

Poll::poll 需要独占访问权限，因为它接受 &mut self，所以当我们在 Poll::poll 中等待事件时，如果我们依赖使用 Poll 来注册兴趣，那么同时从另一个线程注册兴趣是不可能的，因为这会被 Rust 的类型系统阻止。

这也使得在同一个实例上通过调用 Poll::poll 来让多个线程等待事件变得几乎不可能，因为这将需要同步，而这本质上会使每次调用都变成顺序的。

这种设计允许用户从潜在的多个线程与队列交互，通过注册兴趣，而一个线程进行阻塞调用并处理来自操作系统的通知。

注意
mio 不允许你在同一个 Poll::poll 调用上有多个线程被阻塞，这并不是由于 epoll、kqueue 或 IOCP 的限制。它们都允许多个线程在同一个实例上调用 Poll::poll 并获取队列中的事件通知。epoll 甚至允许特定的标志来指示操作系统是否应该只唤醒一个或所有等待通知的线程（特别是 EPPOLLEXCLUSIVE 标志）。

这个问题部分是关于不同平台如何决定唤醒哪些线程，当有许多线程在同一个队列上等待事件时，部分是关于似乎对这种功能没有太大兴趣的事实。例如，epoll 默认会唤醒所有阻塞在 Poll 上的线程，而 Windows 默认只会唤醒一个线程。你可以在一定程度上修改这种行为，并且未来也有在 Poll 上实现 try_clone 方法的想法。目前，设计就像我们概述的那样，我们也会在我们的示例中坚持这一点。

这让我们来到了另一个话题，我们应该在开始实现我们的示例之前讨论它。


* 所有 I/O 操作都是阻塞的吗？
最后，这是一个容易回答的问题。答案是响亮的……也许。问题是，并非所有的 I/O 操作都会以操作系统挂起调用线程的方式进行阻塞，并且切换到另一个任务会更高效。原因是操作系统很聪明，会在内存中缓存大量信息。如果信息在缓存中，请求该信息的系统调用将立即返回数据，因此强制上下文切换或重新调度当前任务可能比同步处理数据效率更低。问题在于，无法确定 I/O 是否会阻塞，这取决于你在做什么。

让我给你两个例子。

DNS 查找
在创建 TCP 连接时，首先需要将典型地址（如 www.google.com）转换为 IP 地址（如 216.58.207.228）。操作系统维护了一个本地地址和之前查找过的地址的映射缓存，几乎可以立即解析它们。然而，第一次查找未知地址时，可能必须调用 DNS 服务器，这需要很长时间，如果未以非阻塞方式处理，操作系统将挂起调用线程以等待响应。

文件 I/O
本地文件系统上的文件是操作系统进行大量缓存的另一个领域。经常读取的较小文件通常缓存在内存中，因此请求该文件可能根本不会阻塞。如果你有一个提供静态文件的 Web 服务器，很可能你提供的是一组相当有限的小文件。这些文件很可能缓存在内存中。然而，无法确定——如果操作系统内存不足，可能必须将内存页面映射到硬盘，这使得通常非常快速的内存查找变得极其缓慢。如果你随机访问大量小文件，或者提供非常大的文件，也会遇到同样的情况，因为操作系统只会缓存有限的信息。如果你在同一操作系统上运行许多不相关的进程，也可能会遇到这种不可预测性，因为它可能不会缓存对你重要的信息。

处理这些情况的一种流行方法是忘记非阻塞 I/O，实际上改为进行阻塞调用。你不希望在运行 Poll 实例的同一线程中进行这些调用（因为每个小的延迟都会阻塞所有任务），但你可能会将该任务委托给线程池。在线程池中，你有有限数量的线程，负责进行常规的阻塞调用，例如 DNS 查找或文件 I/O。

一个完全这样做的运行时的例子是 libuv（http://docs.libuv.org/en/v1.x/threadpool.html#threadpool）。libuv 是 Node.js 构建的异步 I/O 库。虽然它的范围比 mio 更大（mio 只关心非阻塞 I/O），但 libuv 在 JavaScript 中对 Node 的作用，就像 mio 在 Rust 中对 Tokio 的作用一样。



注意
在线程池中执行文件 I/O 的原因是，历史上跨平台的非阻塞文件 I/O API 表现不佳。虽然许多运行时确实选择将此任务委托给线程池，向操作系统发出阻塞调用，但随着操作系统 API 的不断发展，这种情况在未来可能不再适用。创建一个线程池来处理这些情况超出了本示例的范围（即使 mio 也认为这超出了其范围，只是为了明确）。我们将重点展示 epoll 的工作原理，并在文本中提到这些主题，尽管我们不会在本示例中实际实现解决方案。

现在我们已经介绍了关于 epoll、mio 以及示例设计的许多基本信息，是时候编写一些代码并亲自看看这一切在实践中是如何工作的了。

ffi 模块
让我们从那些不依赖其他模块的模块开始，然后逐步展开。ffi 模块包含与操作系统通信所需的系统调用和数据结构的映射。我们还将详细介绍 epoll 的工作原理，一旦我们介绍了系统调用。这部分只有几行代码，所以我会将第一部分放在这里，以便更容易跟踪我们在文件中的位置，因为有很多内容需要解释。打开 ffi.rs 文件并编写以下代码：

ch04/a-epoll/src/ffi.rs
pub const EPOLL_CTL_ADD: i32 = 1;
pub const EPOLLIN: i32 = 0x1;
pub const EPOLLET: i32 = 1 << 31;

#[link(name = "c")]
extern "C" {
    pub fn epoll_create(size: i32) -> i32;
    pub fn close(fd: i32) -> i32;
    pub fn epoll_ctl(epfd: i32, op: i32, fd: i32, event: *mut Event) -> i32;
    pub fn epoll_wait(epfd: i32, events: *mut Event, maxevents: i32, timeout: i32) -> i32;
}
你首先会注意到我们声明了几个常量，分别是 EPOLL_CTL_ADD、EPOLLIN 和 EPOLLET。稍后我会解释这些常量的含义。让我们先看看我们需要进行的系统调用。幸运的是，我们已经详细介绍了系统调用，所以你已经了解了 ffi 的基础知识以及为什么我们在前面的代码中链接到 C。


epoll_create 是我们用来创建 epoll 队列的系统调用。你可以在 https://man7.org/linux/man-pages/man2/epoll_create.2.html 找到它的文档。这个方法接受一个名为 size 的参数，但 size 参数仅出于历史原因存在。该参数会被忽略，但必须大于 0。

close 是我们用来关闭在创建 epoll 实例时获得的文件描述符的系统调用，以便正确释放资源。你可以在 https://man7.org/linux/man-pages/man2/close.2.html 阅读该调用的文档。

epoll_ctl 是我们用来对 epoll 实例执行操作的控制接口。这是我们用来注册对某个事件源感兴趣的事件时所使用的调用。它支持三种主要操作：添加、修改或删除。第一个参数 epfd 是我们想要执行操作的 epoll 文件描述符。第二个参数 op 是我们指定要执行添加、修改还是删除操作的参数。在我们的例子中，我们只对添加事件感兴趣，因此我们只传递 EPOLL_CTL_ADD，这是表示我们要执行添加操作的值。

epoll_event 稍微复杂一些，因此我们将更详细地讨论它。它为我们做了两件重要的事情：首先，events 字段表示我们想要被通知的事件类型，它还可以修改我们如何以及何时收到通知的行为。其次，data 字段向内核传递一段数据，当事件发生时，内核会将此数据返回给我们。后者很重要，因为我们需要这些数据来准确识别发生了什么事件，因为这是我们唯一会收到的信息，可以用来识别通知的来源。你可以在 https://man7.org/linux/man-pages/man2/epoll_ctl.2.html 找到该调用的文档。

epoll_wait 是阻塞当前线程并等待以下两种情况之一的调用：我们收到事件发生的通知，或者超时。epfd 是标识我们通过 epoll_create 创建的队列的 epoll 文件描述符。events 是一个与我们在 epoll_ctl 中使用的相同 Event 结构的数组。不同之处在于，events 字段现在为我们提供了有关发生的事件的信息，重要的是 data 字段包含我们在注册兴趣时传递的相同数据。例如，data 字段让我们可以识别哪个文件描述符有数据可以读取。maxevents 参数告诉内核我们在数组中预留了多少事件的空间。最后，timeout 参数告诉内核我们在等待事件时最多等待多长时间，然后它会再次唤醒我们，以免我们可能永远阻塞。你可以在 https://man7.org/linux/man-pages/man2/epoll_wait.2.html 阅读 epoll_wait 的文档。

该文件中代码的最后一部分是 Event 结构体：

ch04/a-epoll/src/ffi.rs
#[derive(Debug)]
#[repr(C, packed)]
pub struct Event {
    pub(crate) events: u32,
    // Token to identify event
    pub(crate) epoll_data: usize,
}

impl Event {
    pub fn token(&self) -> usize {
        self.epoll_data
    }
}
这个结构体用于在 epoll_ctl 中与操作系统通信，操作系统在 epoll_wait 中使用相同的结构体与我们通信。

events 被定义为一个 u32，但它不仅仅是一个数字。这个字段是我们所说的位掩码（bitmask）。我稍后会花时间解释位掩码，因为它在大多数系统调用中很常见，并不是每个人都遇到过。简单来说，它是一种使用二进制位表示一组是/否标志的方式，用来指示是否选择了某个选项。

不同的选项在我提供的 epoll_ctl 系统调用链接中有描述。我不会在这里详细解释所有选项，只介绍我们将使用的选项：

EPOLLIN 表示一个位标志，表示我们对文件句柄上的读操作感兴趣。
EPOLLET 表示一个位标志，表示我们希望以边缘触发模式接收事件通知。


Event 结构体的最后一个字段是 epoll_data。在文档中，这个字段被定义为一个联合体（union）。联合体很像枚举（enum），但与 Rust 的枚举不同，它不携带任何关于其类型的信息，因此我们需要确保我们知道它持有的数据类型。

我们使用这个字段来简单地保存一个 usize，以便在使用 epoll_ctl 注册兴趣时传递一个整数来标识每个事件。传递一个指针也是完全可以的——只要我们确保在 epoll_wait 中返回时指针仍然有效。

我们可以将这个字段视为一个令牌（token），这正是 mio 所做的。为了尽可能保持 API 的相似性，我们模仿 mio 并在结构体上提供一个 token 方法来获取这个值。

#[repr(packed)] 的作用是什么？
#[repr(packed)] 注解对我们来说是新的。通常，结构体在字段之间或结构体末尾会有填充（padding）。即使我们指定了 #[repr(C)]，这种情况也会发生。

原因与高效访问存储在结构体中的数据有关，通过不需要多次获取来访问存储在结构体字段中的数据。对于 Event 结构体，通常的填充会在 events 字段的末尾添加 4 字节的填充。当操作系统期望一个紧凑的 Event 结构体时，而我们给它一个填充过的结构体，它将会把 event_data 的部分写入字段之间的填充中。当你稍后尝试读取 event_data 时，你最终只会读取 event_data 的最后部分，这部分恰好重叠并导致读取错误的数据。


操作系统期望一个紧凑的 Event 结构体这一事实并不明显，仅通过阅读 Linux 的手册页是无法得知的，因此你必须阅读适当的 C 头文件才能确定。当然，你也可以简单地依赖 libc crate（https://github.com/rust-lang/libc），如果我们不是为了自己学习这些东西，我们也会这样做。

现在我们已经完成了代码的讲解，有几个话题我们之前承诺会回过头来讨论。

位标志（Bitflags）和位掩码（Bitmasks）
在进行系统调用时，你会经常遇到这个概念（事实上，位掩码的概念在低级编程中非常常见）。位掩码是一种将每个位视为开关或标志的方式，用于指示某个选项是启用还是禁用。


一个整数，比如 i32，可以用 32 位来表示。EPOLLIN 的十六进制值是 0x1（十进制就是 1）。用位表示的话，它看起来像 00000000000000000000000000000001。另一方面，EPOLLET 的值是 1 << 31。这意味着将十进制数 1 的位表示向左移动 31 位。十进制数 1 恰好与 EPOLLIN 相同，因此通过查看该表示并将位向左移动 31 次，我们得到一个位表示为 10000000000000000000000000000000 的数字。

我们使用位标志的方式是使用 OR 运算符 |，通过将值进行 OR 操作，我们得到一个位掩码，其中每个 OR 操作的标志都被设置为 1。在我们的例子中，位掩码看起来像 10000000000000000000000000000001。

位掩码的接收者（在这种情况下是操作系统）可以执行相反的操作，检查设置了哪些标志，并相应地采取行动。

我们可以用代码创建一个非常简单的示例来展示这在实践中是如何工作的（你可以直接在 Rust Playground 中运行这个代码，或者创建一个新的空项目来进行这样的实验）：

fn main() {
    let bitflag_a: i32 = 1 << 31;
    let bitflag_b: i32 = 0x1;
    let bitmask: i32 = bitflag_a | bitflag_b;
    println!("{bitflag_a:032b}");
    println!("{bitflag_b:032b}");
    println!("{bitmask:032b}");
    check(bitmask);
}

fn check(bitmask: i32) {
    const EPOLLIN: i32 = 0x1;
    const EPOLLET: i32 = 1 << 31;
    const EPOLLONESHOT: i32 = 0x40000000;
    let read = bitmask & EPOLLIN != 0;
    let et = bitmask & EPOLLET != 0;
    let oneshot = bitmask & EPOLLONESHOT != 0;
    println!("read_event? {read}, edge_triggered: {et}, oneshot?: {oneshot}")
}
这段代码将输出以下内容：

10000000000000000000000000000000
00000000000000000000000000000001
10000000000000000000000000000001
read_event? true, edge_triggered: true, oneshot?: false
在本章中，我们将介绍的下一个主题是边缘触发事件的概念，这可能需要一些解释。

水平触发与边缘触发事件
在一个理想的世界里，我们不需要讨论这个问题，但在使用 epoll 时，几乎不可能避免了解这两者之间的区别。通过阅读文档并不明显，尤其是如果你之前没有接触过这些术语。有趣的是，它允许我们在 epoll 中处理事件的方式与硬件级别处理事件的方式之间建立一种平行关系。

epoll 可以以水平触发或边缘触发模式通知事件。如果你的主要经验是使用高级语言编程，这听起来一定非常晦涩（我第一次学习时也是如此），但请耐心听我解释。在 Event 结构体的事件位掩码中，我们设置 EPOLLET 标志以在边缘触发模式下获得通知（如果你不指定任何内容，默认是水平触发）。

这种事件通知和事件处理的建模方式与计算机处理中断的方式有很多相似之处。

水平触发意味着只要中断线上的电信号报告为高电平，那么“事件是否发生”这个问题的答案就是真。如果我们将此转换到我们的示例中，只要与文件句柄关联的缓冲区中有数据，读取事件就已经发生。

在处理中断时，你可以通过服务引发中断的硬件来清除中断，或者你可以屏蔽中断，这只是在明确解除屏蔽之前禁用该线上的中断。

在我们的示例中，我们通过读取缓冲区中的所有数据来清除中断。当缓冲区被清空时，问题的答案变为假。

当使用 epoll 的默认模式（水平触发）时，我们可能会遇到一种情况，即我们在同一事件上收到多个通知，因为我们还没有时间清空缓冲区（记住，只要缓冲区中有数据，epoll 就会一遍又一遍地通知你）。当我们有一个线程报告事件，然后将处理事件的任务（从流中读取）委托给其他工作线程时，这一点尤其明显，因为 epoll 会愉快地报告事件已准备好，即使我们正在处理它。

为了解决这个问题，epoll 有一个名为 EPOLLONESHOT 的标志。EPOLLONESHOT 告诉 epoll，一旦我们在这个文件描述符上接收到一个事件，它应该禁用兴趣列表中的该文件描述符。它不会移除它，但我们不会再收到该文件描述符的任何通知，除非我们通过调用 epoll_ctl 并使用 EPOLL_CTL_MOD 参数和一个新的位掩码显式重新激活它。

如果我们没有添加这个标志，可能会发生以下情况：如果线程 1 是我们调用 epoll_wait 的线程，那么一旦它接收到关于读取事件的通知，它就会启动线程 2 中的任务来从该文件描述符读取数据，然后再次调用 epoll_wait 以获取新事件的通知。在这种情况下，epoll_wait 的调用将再次返回，并告诉我们同一文件描述符上的数据已准备好，因为我们还没有时间清空该文件描述符的缓冲区。我们知道任务由线程 2 处理，但我们仍然会收到通知。如果没有额外的同步和逻辑，我们可能会将读取同一文件描述符的任务交给线程 3，这可能会导致一些非常难以调试的问题。

使用 EPOLLONESHOT 可以解决这个问题，因为线程 2 在处理完任务后必须重新激活事件队列中的文件描述符，从而告诉我们的 epoll 队列它已经完成了任务，并且我们再次对该文件描述符的通知感兴趣。

回到我们最初的中断类比，EPOLLONESHOT 可以被认为是屏蔽中断。你还没有真正清除事件通知的源头，但你不想在完成并显式解除屏蔽之前收到进一步的通知。在 epoll 中，EPOLLONESHOT 标志将禁用文件描述符上的通知，直到你通过调用 epoll_ctl 并将 op 参数设置为 EPOLL_CTL_MOD 来显式启用它。

边缘触发意味着“事件是否发生”这个问题的答案只有在电信号从低电平变为高电平时才为真。如果我们将此转换到我们的示例中：当缓冲区从没有数据变为有数据时，读取事件已经发生。只要缓冲区中有数据，就不会报告新的事件。你仍然通过从套接字中读取所有数据来处理事件，但在缓冲区完全清空并再次填充新数据之前，你不会收到新的通知。

边缘触发模式也有一些陷阱。最大的一个问题是，如果你没有正确清空缓冲区，你将永远不会再收到该文件句柄的通知。


图 4.1 – 边缘触发与水平触发事件

在撰写本文时，mio 并不支持 EPOLLONESHOT，并且以边缘触发模式使用 epoll，我们在示例中也会这样做。

关于在多个线程中等待 epoll_wait 的问题

只要我们只有一个 Poll 实例，就可以避免多个线程在同一 epoll 实例上调用 epoll_wait 的问题和复杂性。使用水平触发事件会唤醒所有在 epoll_wait 调用中等待的线程，导致它们都尝试处理事件（这通常被称为“惊群问题”）。epoll 还有一个你可以设置的标志，称为 EPOLLEXCLUSIVE，可以解决这个问题。默认情况下，设置为边缘触发的事件只会唤醒一个在 epoll_wait 中阻塞的线程，从而避免这个问题。

由于我们只从单个线程使用一个 Poll 实例，这对我们来说不会成为问题。

我知道并理解这听起来非常复杂。事件队列的一般概念相当简单，但细节可能会有些复杂。话虽如此，epoll 是我经验中最复杂的 API 之一，因为该 API 显然随着时间的推移不断演变，以适应现代需求，并且如果不至少涵盖我们在这里讨论的主题，实际上没有简单的方法可以正确使用和理解它。

这里有一点安慰是，kqueue 和 IOCP 的 API 更容易理解。此外，Unix 有一个新的异步 I/O 接口，称为 io_uring，它在未来会变得越来越普遍。

现在我们已经介绍了本章的难点，并对 epoll 的工作原理有了一个高层次的概述，是时候在 poll.rs 中实现我们受 mio 启发的 API 了。

Poll 模块

如果你还没有编写或复制我们在“设计和 epoll 介绍”部分中提供的代码，现在是时候这样做了。我们将实现所有之前只有 todo!() 的函数。

我们首先在 Poll 结构体上实现方法。首先是打开 impl Poll 块并实现 new 函数：

ch04/a-epoll/src/poll.rs
impl Poll {
    pub fn new() -> Result<Self> {
        let res = unsafe { ffi::epoll_create(1) };
        if res < 0 {
            return Err(io::Error::last_os_error());
        }
        Ok(Self {
            registry: Registry { raw_fd: res },
        })
    }
鉴于在“ffi 模块”部分中对 epoll 的详细介绍，这应该相当简单。我们调用 ffi::epoll_create，参数为 1（记住，该参数被忽略，但必须具有非零值）。如果我们得到任何错误，我们会要求操作系统报告我们进程的最后一个错误并返回该错误。如果调用成功，我们返回一个新的 Poll 实例，它只是包装了包含 epoll 文件描述符的 Registry。

接下来是我们的 registry 方法，它只是返回对内部 Registry 结构体的引用：

ch04/a-epoll/src/poll.rs
    pub fn registry(&self) -> &Registry {
        &self.registry
    }
Poll 上的最后一个方法是最有趣的一个。它是 poll 函数，它将暂停当前线程，并告诉操作系统在我们跟踪的源上发生事件或超时（以先到者为准）时唤醒它。我们在这里也关闭了 impl Poll 块：

ch04/a-epoll/src/poll.rs
    pub fn poll(&mut self, events: &mut Events, timeout: Option<i32>) -> Result<()> {
        let fd = self.registry.raw_fd;
        let timeout = timeout.unwrap_or(-1);
        let max_events = events.capacity() as i32;
        let res = unsafe { ffi::epoll_wait(fd, events.as_mut_ptr(), max_events, timeout) };
        if res < 0 {
            return Err(io::Error::last_os_error());
        };
        unsafe { events.set_len(res as usize) };
        Ok(())
    }
}
我们做的第一件事是获取事件队列的原始文件描述符并将其存储在 fd 变量中。


接下来是我们的超时设置。如果它是 Some，我们解包该值；如果它是 None，我们将其设置为 -1，这个值告诉操作系统我们希望阻塞直到事件发生，即使这可能永远不会发生。

在文件的顶部，我们将 Events 定义为 Vec<ffi::Event> 的类型别名，因此接下来我们要做的是获取该 Vec 的容量。重要的是我们不要依赖 Vec::len，因为它报告的是 Vec 中有多少项。Vec::capacity 报告我们分配的空间，而这正是我们所需要的。

接下来是调用 ffi::epoll_wait。如果返回值大于或等于 0，该调用将成功返回，告诉我们发生了多少事件。

注意：如果在事件发生之前超时，我们将得到值为 0。

我们做的最后一件事是对 events.set_len(res as usize) 进行不安全调用。这个函数是不安全的，因为我们可能会设置长度，从而在安全的 Rust 中访问尚未初始化的内存。我们从操作系统提供的保证中知道，它返回的事件数量指向我们 Vec 中的有效数据，因此在这种情况下这是安全的。

接下来是我们的 Registry 结构体。我们只实现一个名为 register 的方法，最后，我们为其实现 Drop trait，关闭 epoll 实例：

ch04/a-epoll/src/poll.rs
impl Registry {
    pub fn register(&self, source: &TcpStream, token: usize, interests: i32) -> Result<()> {
        let mut event = ffi::Event {
            events: interests as u32,
            epoll_data: token,
        };
        let op = ffi::EPOLL_CTL_ADD;
        let res = unsafe {
            ffi::epoll_ctl(self.raw_fd, op, source.as_raw_fd(), &mut event)
        };
        if res < 0 {
            return Err(io::Error::last_os_error());
        }
        Ok(())
    }
}
register 函数接受一个 &TcpStream 作为源，一个 usize 类型的 token，以及一个名为 interests 的位掩码，其类型为 i32。

注意：这是 mio 做不同事情的地方。source 参数是特定于每个平台的。register 的实现不是在 Registry 上处理的，而是在它接收的 source 参数中以平台特定的方式处理的。

我们做的第一件事是创建一个 ffi::Event 对象。events 字段简单地设置为我们接收到的位掩码 interests，而 epoll_data 设置为我们传入的 token 参数的值。



我们希望在 epoll 队列上执行的操作是添加对新文件描述符事件的兴趣。因此，我们将 op 参数设置为 ffi::EPOLL_CTL_ADD 常量值。

接下来是调用 ffi::epoll_ctl。我们首先传入 epoll 实例的文件描述符，然后传入 op 参数以指示我们要执行的操作类型。最后两个参数是我们希望队列跟踪的文件描述符和我们创建的 Event 对象，用于指示我们感兴趣的事件类型。

函数体的最后一部分是错误处理，这部分现在应该已经很熟悉了。

poll.rs 的最后一部分是 Registry 的 Drop 实现：

ch04/a-epoll/src/poll.rs
impl Drop for Registry {
    fn drop(&mut self) {
        let res = unsafe { ffi::close(self.raw_fd) };
        if res < 0 {
            let err = io::Error::last_os_error();
            eprintln!("ERROR: {err:?}");
        }
    }
}
Drop 实现简单地调用了 ffi::close 来关闭 epoll 文件描述符。在 drop 中添加 panic 通常不是一个好主意，因为 drop 可能已经在 panic 中被调用，这会导致进程直接中止。mio 在其 Drop 实现中记录错误，但不会以其他方式处理它们。对于我们这个简单的示例，我们只是打印错误，以便我们可以看到是否出现问题，因为我们在这里没有实现任何日志记录。

最后一部分是运行我们示例的代码，这引导我们进入 main.rs。

主程序
让我们看看这一切在实践中是如何工作的。确保 delayserver 已启动并运行，因为我们需要它才能使这些示例正常工作。

目标是向 delayserver 发送一组具有不同延迟的请求，然后使用 epoll 等待响应。因此，在此示例中，我们仅使用 epoll 来跟踪读取事件。目前，该程序的功能仅限于此。

我们要做的第一件事是确保我们的 main.rs 文件设置正确：

ch04/a-epoll/src/main.rs
use std::{io::{self, Read, Result, Write}, net::TcpStream};
use ffi::Event;
use poll::Poll;

mod ffi;
mod poll;
我们从自己的 crate 和标准库中导入了一些类型，这些类型将在后续使用，同时我们还声明了两个模块。

在此示例中，我们将直接使用 TcpStream，这意味着我们必须自己格式化向 delayserver 发出的 HTTP 请求。

服务器将接受 GET 请求，因此我们创建了一个小的辅助函数来为我们格式化有效的 HTTP GET 请求：

ch04/a-epoll/src/main.rs
fn get_req(path: &str) -> Vec<u8> {
    format!(
        "GET {path} HTTP/1.1\r\n\
         Host: localhost\r\n\
         Connection: close\r\n\
         \r\n"
    ).into_bytes()
}
前面的代码简单地接受一个路径作为输入参数，并使用它格式化一个有效的 GET 请求。路径是 URL 中方案和主机之后的部分。在我们的例子中，路径将是以下 URL 中加粗的部分：http://localhost:8080/2000/hello-world。

接下来是我们的 main 函数。它分为两部分：

设置和发送请求
等待并处理传入事件
main 函数的第一部分如下所示：


fn main() -> Result<()> {
    let mut poll = Poll::new()?;
    let n_events = 5;
    let mut streams = vec![];
    let addr = "localhost:8080";
    for i in 0..n_events {
        let delay = (n_events - i) * 1000;
        let url_path = format!("/{delay}/request-{i}");
        let request = get_req(&url_path);
        let mut stream = std::net::TcpStream::connect(addr)?;
        stream.set_nonblocking(true)?;
        stream.write_all(request.as_bytes())?;
        poll.registry()
            .register(&stream, i, ffi::EPOLLIN | ffi::EPOLLET)?;
        streams.push(stream);
    }
我们要做的第一件事是创建一个新的 Poll 实例。我们还指定了在示例中要创建和处理的事件数量。

下一步是创建一个变量来保存 Vec<TcpStream> 对象的集合。我们还将本地 delayserver 的地址存储在一个名为 addr 的变量中。

接下来的部分是我们创建一组请求并将其发送到 delayserver，最终 delayserver 会响应我们。对于每个请求，我们期望稍后在发送请求的 TcpStream 上发生读取事件。

在循环中，我们做的第一件事是设置延迟时间（以毫秒为单位）。将延迟设置为 (n_events - i) * 1000 意味着我们发出的第一个请求将具有最长的超时时间，因此我们应该期望响应以与发送顺序相反的顺序到达。

注意：为了简单起见，我们使用事件在 streams 集合中的索引作为其 ID。此 ID 将与循环中的 i 变量相同。例如，在第一次循环中，i 将为 0；它也将是第一个被推送到 streams 集合中的流，因此索引也将为 0。因此，我们始终使用 0 作为此流/事件的标识符，因为检索与此事件关联的 TcpStream 只需在 streams 集合中索引到该位置即可。

下一行 format!("/{delay}/request-{i}") 格式化我们的 GET 请求的路径。我们按照前面描述的方式设置超时，并且还设置了一个消息，其中存储了此事件的标识符 i，以便我们也可以在服务器端跟踪此事件。

接下来是创建 TcpStream。你可能已经注意到，Rust 中的 TcpStream 不接受 &str，而是接受实现了 ToSocketAddrs 特征的参数。该特征已经为 &str 实现，因此我们可以像在此示例中那样简单地编写它。

在 TcpStream::connect 实际打开套接字之前，它会尝试将我们传递的地址解析为 IP 地址。如果失败，它将将其解析为域名和端口号，然后要求操作系统对该地址进行 DNS 查找，然后才能实际连接到我们的服务器。因此，你可以看到，当我们进行简单的连接时，可能会发生相当多的事情。

你可能还记得我们之前讨论过 DNS 查找的一些细微差别，以及这样一个调用可能非常快（因为操作系统已经将信息存储在内存中）或阻塞（等待 DNS 服务器的响应）的事实。如果你希望完全控制整个过程，使用标准库中的 TcpStream 可能会带来潜在的缺点。


Rust 中的 TcpStream 和 Nagle 算法
这里有一个小事实（我原本想称之为“有趣的事实”，但后来意识到这有点过于牵强了！）。在 Rust 的 TcpStream 中，更重要的是，大多数旨在模仿标准库 TcpStream 的 API（如 mio 或 Tokio），流在创建时默认将 TCP_NODELAY 标志设置为 false。实际上，这意味着使用了 Nagle 算法，这可能会导致一些延迟异常问题，并在某些工作负载下可能降低吞吐量。

Nagle 算法是一种旨在通过将小型网络数据包合并在一起来减少网络拥塞的算法。如果你查看其他语言中的非阻塞 I/O 实现，许多（如果不是大多数）默认情况下会禁用此算法。然而，在大多数 Rust 实现中并非如此，这一点值得注意。你可以通过简单地调用 TcpStream::set_nodelay(true) 来禁用它。如果你尝试创建自己的异步库或依赖 Tokio/mio，并观察到吞吐量低于预期或延迟问题，值得检查此标志是否设置为 true。

继续代码部分，下一步是通过调用 TcpStream::set_nonblocking(true) 将 TcpStream 设置为非阻塞模式。之后，我们通过设置 EPOLLIN 标志位在兴趣位掩码中注册对读取事件的兴趣，然后将请求写入服务器。

在每次迭代中，我们将流推送到 streams 集合的末尾。

接下来是主函数中处理传入事件的部分。让我们看一下主函数的最后一部分：

let mut handled_events = 0;
while handled_events < n_events {
    let mut events = Vec::with_capacity(10);
    poll.poll(&mut events, None)?;
    if events.is_empty() {
        println!("TIMEOUT (OR SPURIOUS EVENT NOTIFICATION)");
        continue;
    }
    handled_events += handle_events(&events, &mut streams)?;
}
println!("FINISHED");
Ok(())
我们做的第一件事是创建一个名为 handled_events 的变量，用于跟踪我们已经处理了多少事件。

接下来是我们的 事件循环。只要处理的事件少于我们预期的事件数量，我们就会继续循环。一旦所有事件都被处理完毕，我们就退出循环。


循环内部的操作
在循环内部，我们创建了一个 Vec<Event>，其容量为存储 10 个事件。重要的是，我们使用 Vec::with_capacity 来创建它，因为操作系统会假设我们传递给它的是已经分配的内存。我们可以选择任意数量的事件，这都能正常工作，但如果设置得太低，会限制操作系统每次唤醒时可以通知我们的事件数量。

接下来是我们对 Poll::poll 的阻塞调用。如你所知，这实际上会告诉操作系统暂停我们的线程，并在事件发生时唤醒我们。

如果我们被唤醒，但事件列表中没有事件，那么这可能是超时或虚假事件（这种情况可能会发生，因此如果对我们来说很重要，我们需要一种方法来检查是否真的发生了超时）。如果是这种情况，我们只需再次调用 Poll::poll。

如果有事件需要处理，我们将这些事件与 streams 集合的可变引用一起传递给 handle_events 函数。

主函数的最后部分只是简单地向控制台写入 FINISHED，以让我们知道我们在这一点上退出了主函数。

本章的最后一段代码是 handle_events 函数。该函数接受两个参数：一个 Event 结构体的切片和一个 TcpStream 对象的可变切片。

让我们在解释之前先看一下代码：

fn handle_events(events: &[Event], streams: &mut [TcpStream]) -> Result<usize> {
    let mut handled_events = 0;
    for event in events {
        let index = event.token();
        let mut data = vec![0u8; 4096];
        loop {
            match streams[index].read(&mut data) {
                Ok(n) if n == 0 => {
                    handled_events += 1;
                    break;
                }
                Ok(n) => {
                    let txt = String::from_utf8_lossy(&data[..n]);
                    println!("RECEIVED: {:?}", event);
                    println!("{txt}\n------\n");
                }
                // 非阻塞模式下尚未准备好读取。即使事件被报告为就绪，这也可能发生
                Err(e) if e.kind() == io::ErrorKind::WouldBlock => break,
                Err(e) => return Err(e),
            }
        }
    }
    Ok(handled_events)
}
我们做的第一件事是创建一个变量 handled_events，用于跟踪每次唤醒时我们认为已经处理了多少事件。下一步是遍历我们收到的事件。

在循环中，我们检索标识我们收到事件的 TcpStream 的令牌。正如我们在这个示例中之前解释的那样，这个令牌与 streams 集合中特定流的索引相同，因此我们可以简单地使用它来索引到我们的 streams 集合并检索正确的 TcpStream。

在我们开始读取数据之前，我们创建了一个大小为 4,096 字节的缓冲区（当然，如果你愿意，可以分配更大或更小的缓冲区）。

我们创建了一个循环，因为我们可能需要多次调用 read 以确保我们确实已经排空了缓冲区。请记住，在使用边缘触发模式的 epoll 时，完全排空缓冲区是多么重要。

我们根据调用 TcpStream::read 的结果进行匹配，因为我们希望根据结果采取不同的操作。



处理读取结果
如果我们得到 Ok(n) 并且值为 0，说明我们已经排空了缓冲区；我们将该事件视为已处理，并跳出循环。

如果我们得到 Ok(n) 并且值大于 0，我们将数据读取到一个 String 中，并以一定的格式打印出来。我们暂时不会跳出循环，因为我们必须继续调用 read，直到返回 0（或发生错误），以确保我们已经完全排空了缓冲区。

如果我们得到 Err 并且错误类型为 io::ErrorKind::WouldBlock，我们只需跳出循环。我们暂时不认为该事件已处理，因为 WouldBlock 表示数据传输尚未完成，但目前没有数据准备好。

如果我们得到任何其他错误，我们直接返回该错误并将其视为失败。

注意：

通常你还需要处理另一种错误情况，即 io::ErrorKind::Interrupted。从流中读取数据可能会被操作系统的信号中断。这种情况应该被预期，并且通常不应被视为失败。处理这种情况的方式与处理 WouldBlock 类型的错误相同。

如果读取操作成功，我们返回已处理的事件数量。


使用 TcpStream::read_to_end 时要小心
在使用 TcpStream::read_to_end 或任何其他为你完全排空缓冲区的函数时，要特别小心，尤其是在使用非阻塞缓冲区的情况下。如果你遇到 io::ErrorKind::WouldBlock 类型的错误，它会报告为错误，即使在此之前你已经成功读取了多次。你无法知道成功读取了多少数据，除非观察传递给函数的 &mut Vec 的变化。

现在，如果我们运行我们的程序，应该会得到以下输出：

RECEIVED: Event { events: 1, epoll_data: 4 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:09 GMT
request-4
------
RECEIVED: Event { events: 1, epoll_data: 3 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:10 GMT
request-3
------
RECEIVED: Event { events: 1, epoll_data: 2 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:11 GMT
request-2
------
RECEIVED: Event { events: 1, epoll_data: 1 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:12 GMT
request-1
------
RECEIVED: Event { events: 1, epoll_data: 0 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:13 GMT
request-0
------
FINISHED
如你所见，响应是以相反的顺序发送的。你可以通过查看运行 delayserver 实例时的终端输出来轻松确认这一点。输出应该如下所示：

#1 - 5000ms: request-0
#2 - 4000ms: request-1
#3 - 3000ms: request-2
#4 - 2000ms: request-3
#5 - 1000ms: request-4
有时顺序可能会有所不同，因为服务器几乎同时接收到它们，并可能选择以略微不同的顺序处理它们。

假设我们跟踪 ID 为 4 的流上的事件
在 send_requests 中，我们将 ID 4 分配给我们创建的最后一个流。
套接字 4 向 delayserver 发送一个请求，设置 1,000 毫秒的延迟，并发送消息 request-4，以便我们可以在服务器端识别它。
我们将套接字 4 注册到事件队列中，确保将 epoll_data 字段设置为 4，以便我们可以识别事件发生在哪个流上。
delayserver 接收到该请求，并在发送 HTTP/1.1 200 OK 响应之前延迟 1,000 毫秒，同时返回我们最初发送的消息。
epoll_wait 唤醒，通知我们有一个事件准备就绪。在 Event 结构体的 epoll_data 字段中，我们得到与注册事件时传入的相同数据。这告诉我们事件发生在流 4 上。
然后我们从流 4 读取数据并打印出来。
在这个例子中，尽管我们使用了标准库来处理建立连接的复杂性，但我们仍然保持了非常底层的操作。尽管你实际上已经向本地服务器发出了一个原始的 HTTP 请求，但你设置了一个 epoll 实例来跟踪 TcpStream 上的事件，并使用 epoll 和系统调用来处理传入的事件。

这可不是一件小事——恭喜你！

在我们离开这个例子之前，我想指出，为了使我们的例子使用 mio 作为事件循环，而不是我们自己创建的那个，我们需要做的改动非常少。在 ch04/b-epoll-mio 目录下的代码库中，你会看到一个我们使用 mio 做完全相同事情的例子。它只需要从 mio 导入一些类型，而不是我们自己的模块，并且只需要对我们的代码进行五个小的改动！

你不仅复制了 mio 的功能，而且几乎也知道了如何使用 mio 来创建一个事件循环！

总结
epoll、kqueue 和 IOCP 的概念在高层面上非常简单，但魔鬼藏在细节中。要正确理解并使其工作并不容易。即使是从事这些工作的程序员也通常会专攻一个平台（epoll/kqueue 或 Windows）。很少有人会了解所有平台的所有细节，你甚至可以单独写一整本书来讨论这个主题。

如果我们总结一下你在本章中学到并亲身体验到的内容，这个列表相当令人印象深刻：

你学到了很多关于 mio 是如何设计的，使你能够更容易地进入代码库并知道该寻找什么，以及如何开始阅读代码。
你学到了很多关于在 Linux 上进行系统调用的知识。
你创建了一个 epoll 实例，注册了事件并处理了这些事件。
你学到了很多关于 epoll 的设计及其 API 的知识。
你了解了边缘触发和水平触发，这些是非常底层的概念，但在 epoll 之外的上下文中也非常有用。
你发出了一个原始的 HTTP 请求。
你看到了非阻塞套接字的行为方式，以及操作系统报告的错误代码如何传达某些你应处理的条件。
你通过查看 DNS 解析和文件 I/O，了解到并非所有 I/O 都是“阻塞”的。
对于一个章节来说，这已经相当不错了！

如果你深入研究我们在这里讨论的主题，你很快就会意识到到处都是陷阱和深坑——尤其是如果你将这个例子扩展到抽象 epoll、kqueue 和 IOCP。你可能最终会在不知不觉中阅读 Linus Torvalds 关于边缘触发模式在管道上应该如何工作的电子邮件。

至少你现在有了进一步探索的良好基础。你可以扩展我们的简单示例，创建一个适当的事件循环来处理连接、写入、超时和调度；你可以深入研究 kqueue 和 IOCP，看看 mio 是如何解决这个问题的；或者你可以庆幸自己不必再直接处理这些问题，并欣赏像 mio、polling 和 libuv 这样的库所付出的努力。

到这个时候，我们已经获得了关于异步编程基本构建模块的很多知识，所以是时候开始探索不同的编程语言如何创建异步操作的抽象，并使用这些构建模块为我们程序员提供高效、表达力强且富有成效的方式来编写异步程序了。

首先是我最喜欢的一个例子，我们将通过自己实现纤程（或绿色线程）来了解它们的工作原理。

你现在可以休息一下了
是的，继续吧，下一章可以等一等。去喝杯茶或咖啡，放松一下，这样你可以以清醒的头脑开始下一章。我保证它会既有趣又有意思。


创建我们自己的纤程
在本章中，我们将深入探讨一种非常流行的处理并发的方式。要获得对这一主题的基本理解，没有比自己动手更好的方法了。幸运的是，尽管这个话题有点复杂，但我们只需要大约 200 行代码就能最终获得一个完整的可运行示例。

使这个主题复杂的原因是，它需要对 CPU、操作系统和汇编的工作原理有相当多的基本理解。这种复杂性也是使这个主题如此有趣的原因。如果你详细探索并完成这个示例，你将会获得对某些主题的深刻理解，这些主题你可能只是听说过或只有初步的了解。你还将有机会了解 Rust 语言中一些你以前未见过的方面，从而扩展你对 Rust 和编程的整体知识。

我们首先介绍一些在开始编写代码之前需要的背景知识。一旦我们掌握了这些知识，我们将从一些小示例开始，这些示例将允许我们详细展示和讨论示例中最技术和最困难的部分，以便我们可以逐步引入这些主题。最后，我们将基于所获得的知识，创建我们的主要示例，这是一个用 Rust 实现的纤程的工作示例。

作为奖励，你将在代码库中获得两个扩展版本的示例，以激励你继续修改、调整和构建我们所创建的内容，使其成为你自己的作品。

我将在这里列出主要主题，以便你以后可以参考它们：

如何与本书一起使用代码库
背景信息
我们可以构建的示例
栈
实现我们自己的纤程
最后的思考
注意
在本章中，我们将使用术语“纤程”和“绿色线程”来指代这种具体的栈式协程实现。本章中使用的术语“线程”将指我们在示例中实现的绿色线程/纤程，而不是操作系统线程。

技术要求
要运行这些示例，你需要一台使用 x86-64 指令集的 CPU 的计算机。目前大多数流行的台式机、服务器和笔记本电脑 CPU 都使用这种指令集，大多数现代 Intel 和 AMD 的 CPU 也是如此（这些制造商在过去 10-15 年内生产的大多数 CPU 型号都使用这种指令集）。

一个需要注意的是，现代的 M 系列 Mac 使用 ARM ISA（指令集），这与我们在这里编写的示例不兼容。然而，较旧的基于 Intel 的 Mac 是兼容的，因此如果你没有最新版本的 Mac，你应该仍然可以使用 Mac 来跟随学习。

如果你没有使用这种指令集的计算机，你有几种选择来安装 Rust 并运行这些示例：

使用 M 系列芯片的 Mac 用户可以使用 Rosetta（随新版 MacOS 提供），只需四个简单步骤即可使示例正常工作。你可以在代码库中的 ch05/How-to-MacOS-M.md 下找到说明。
使用虚拟机（如 https://mac.getutm.app/，有些甚至提供免费层）。
使用运行在 x86-64 上的 Linux 远程服务器。我有使用 Linode 的经验（/），但还有很多其他选择。
要跟随本书中的示例，你还需要一个基于 Unix 的操作系统。示例代码将在任何 Linux 和 BSD 操作系统（如 Ubuntu 或 macOS）上本地运行，只要它运行在 x86-64 CPU 上。

如果你使用的是 Windows，代码库中也有一个适用于 Windows 的示例版本，但要跟随本书学习，我明确建议你设置 Windows Subsystem for Linux (WSL)（https://learn.microsoft.com/en-us/windows/wsl/install），安装 Rust，并使用 WSL 上的 Rust 来跟随学习。

我个人使用 VS Code 作为我的编辑器，因为它使得在 WSL 和 Windows 之间切换变得非常容易——只需按下 Ctrl + Shift + P 并搜索“Reopen folder in WSL”。


如何与本书一起使用代码库
推荐阅读本章的方式是同时打开代码库和本书。在代码库中，你将找到三个不同的文件夹，它们对应于我们在本章中讨论的示例：

ch05/a-stack-swap
ch05/b-show-stack
ch05/c-fibers
此外，你还将获得另外两个示例，这些示例在书中提到，但应在代码库中探索：

ch05/d-fibers-closure：这是第一个示例的扩展版本，可能会激励你自己做更复杂的事情。该示例尝试模仿 Rust 标准库中使用的 std::thread::spawn API。
ch05/e-fibers-windows：这是我们在本书中讨论的示例的一个版本，适用于基于 Unix 的系统以及 Windows。在 README 中有相当详细的解释，说明了我们为使示例在 Windows 上工作所做的更改。如果你想深入研究这个主题，我建议你阅读这部分内容，但理解本章中讨论的主要概念并不需要这部分内容。
背景信息
我们将直接干扰和控制 CPU。由于存在许多不同类型的 CPU，这种方法并不具有很好的可移植性。虽然整体实现是相同的，但实现中有一个小而重要的部分将非常特定于我们所编程的 CPU 架构。另一个限制我们代码可移植性的方面是，操作系统有不同的 ABI（应用程序二进制接口），我们需要遵守这些 ABI，而这些相同的代码片段将不得不根据不同的 ABI 进行更改。在我们继续之前，让我们确切解释一下我们在这里的意思，以确保我们在同一页上。

指令集、硬件架构和 ABI
在我们开始之前，我们需要了解应用程序二进制接口（ABI）、CPU 架构和指令集架构（ISA）之间的区别。我们需要这些知识来编写我们自己的栈，并使 CPU 跳转到它。幸运的是，虽然这听起来可能很复杂，但我们只需要知道一些特定的东西就可以让我们的示例运行。这里提供的信息在许多情况下都很有用，而不仅仅是在我们的示例中，因此值得详细讨论。

ISA 描述了 CPU 的抽象模型，定义了 CPU 如何被其运行的软件控制。我们通常简单地将其称为指令集，它定义了 CPU 可以执行的指令、程序员可以使用的寄存器、硬件如何管理内存等。ISA 的例子包括 x86-64、x86 和 ARM ISA（用于 Mac M 系列芯片）。

ISA 根据其复杂性大致分为两个子组：复杂指令集计算机（CISC）和精简指令集计算机（RISC）。CISC 架构提供了许多不同的指令，硬件必须知道如何执行这些指令，导致一些指令非常专业化，程序很少使用。RISC 架构接受的指令较少，但需要一些操作由软件处理，而这些操作在 CISC 架构中可以直接由硬件处理。我们将重点关注的 x86-64 指令集是 CISC 架构的一个例子。



    
